#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/tsan - Automated ThreadSanitizer error fix loop

Runs make check-tsan, fixes data races one at a time with escalation,
commits on success, reverts on exhaustion, continues until all pass or
no progress is made.
"""

import argparse
import subprocess
import sys
import os
import re
import threading
import time
from datetime import datetime
from pathlib import Path

# Configuration
MAX_ATTEMPTS_PER_FILE = 3

# Escalation ladder: (model, thinking_budget, display_name)
ESCALATION_LADDER = {
    1: ("claude-sonnet-4-20250514", "10000", "sonnet:think"),
    2: ("claude-opus-4-20250514", "10000", "opus:think"),
    3: ("claude-opus-4-20250514", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
FIX_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
DEFAULT_TIMEOUT = 900


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'tsan':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None, timeout: int | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
        timeout=timeout,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def drop_test_databases() -> None:
    """Drop all ikigai_test_* databases before running tsan."""
    log("Cleaning up test databases...")
    spinner = Spinner("Dropping test databases")
    spinner.start()

    # Get list of test databases
    code, stdout, stderr = run_cmd([
        "psql",
        "postgresql://ikigai:ikigai@localhost/postgres",
        "-t",  # Tuples only (no headers)
        "-c",
        "SELECT datname FROM pg_database WHERE datname LIKE 'ikigai_test_%';"
    ])

    if code != 0:
        spinner.stop()
        log(f"Warning: Failed to list test databases: {stderr}")
        return

    # Parse database names from output
    db_names = [line.strip() for line in stdout.strip().split('\n') if line.strip()]

    if not db_names:
        spinner.stop()
        log("No test databases to clean up")
        return

    # Drop each database
    dropped_count = 0
    for db_name in db_names:
        code, _, stderr = run_cmd([
            "psql",
            "postgresql://ikigai:ikigai@localhost/postgres",
            "-c",
            f"DROP DATABASE IF EXISTS {db_name};"
        ])
        if code == 0:
            dropped_count += 1

    spinner.stop()
    log(f"Dropped {dropped_count} test database(s)")


def run_make_tsan() -> tuple[bool, str]:
    """Run make check-tsan and return (success, output)."""
    log("Running make check-tsan...")
    spinner = Spinner("Running make check-tsan")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", "check-tsan"], timeout=600)
    spinner.stop()
    output = stdout + stderr
    return code == 0, output


def parse_failures(output: str) -> list[dict]:
    """
    Parse ThreadSanitizer output to extract data races.
    Returns list of {file, line, error_type, message, stack} dicts.
    """
    failures = []
    seen_files = set()

    # ThreadSanitizer data race pattern:
    # WARNING: ThreadSanitizer: data race (pid=...)
    #   Write of size N at ... by thread T1:
    #     #0 function file.c:123 (binary+0x...)
    race_pattern = re.compile(r'WARNING: ThreadSanitizer: (data race|race on [^\n]+)')
    location_pattern = re.compile(r'#\d+\s+\S+\s+(\S+):(\d+)\s+')

    # Also match simpler patterns
    simple_location = re.compile(r'(\S+\.c):(\d+)')

    lines = output.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i]

        race_match = race_pattern.search(line)
        if race_match:
            error_type = race_match.group(1)
            stack_lines = []

            # Collect stack trace (next 30 lines or until empty line)
            for j in range(i + 1, min(i + 30, len(lines))):
                if not lines[j].strip():
                    break
                stack_lines.append(lines[j])

                # Find first source location in our code
                loc_match = location_pattern.search(lines[j])
                if not loc_match:
                    loc_match = simple_location.search(lines[j])

                if loc_match:
                    file = loc_match.group(1)
                    line_num = loc_match.group(2)
                    if file not in seen_files and (file.startswith('src/') or file.startswith('tests/')):
                        seen_files.add(file)
                        failures.append({
                            "file": file,
                            "line": line_num,
                            "error_type": f"TSan: {error_type}",
                            "message": error_type,
                            "stack": '\n'.join(stack_lines[:15]),
                        })
                        break

            i += 1
            continue

        i += 1

    return failures


def load_prompt_template() -> str:
    """Load the fix prompt template."""
    if not FIX_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {FIX_PROMPT_TEMPLATE}")
        sys.exit(1)
    return FIX_PROMPT_TEMPLATE.read_text()


def build_prompt(failure: dict, make_output: str) -> str:
    """Build the fix prompt from template."""
    template = load_prompt_template()

    prompt = template.replace("{{file}}", failure["file"])
    prompt = prompt.replace("{{line}}", failure["line"])
    prompt = prompt.replace("{{error_type}}", failure["error_type"])
    prompt = prompt.replace("{{message}}", failure["message"])
    prompt = prompt.replace("{{stack}}", failure["stack"])
    prompt = prompt.replace("{{make_output}}", make_output[-6000:])

    return prompt


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, response).
    """
    import json
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = f'cat "{prompt_file}" | claude -p - --model {model} --allowedTools "Read,Edit,Write,Bash,Glob,Grep" --output-format json --max-turns 30'
        env = os.environ.copy()
        env["MAX_THINKING_TOKENS"] = thinking_budget
        spinner.start()
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=PROJECT_ROOT,
            timeout=timeout,
            env=env,
        )
        spinner.stop()
        code = result.returncode
        stdout = result.stdout or ""
        stderr = result.stderr or ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"Timeout after {timeout} seconds"
    finally:
        Path(prompt_file).unlink(missing_ok=True)

    if code != 0:
        return False, stderr

    try:
        result = json.loads(stdout)
        return True, result.get("result", "")
    except json.JSONDecodeError:
        return True, stdout


def jj_get_modified_files() -> set[str]:
    """Get set of currently modified files."""
    code, stdout, _ = run_cmd(["jj", "diff", "--summary"])
    files = set()
    for line in stdout.strip().split('\n'):
        if line.strip():
            parts = line.split(None, 1)
            if len(parts) == 2:
                files.add(parts[1])
    return files


def jj_commit(failure: dict, model_name: str, attempt: int, files_before: set[str]) -> bool:
    """Commit only files that changed during fix attempt."""
    files_after = jj_get_modified_files()
    new_changes = files_after - files_before

    if not new_changes:
        return False

    file_name = Path(failure["file"]).name
    msg = f"fix: data race in {file_name}\n\nharness/tsan | {model_name} | attempt {attempt}"

    code, _, _ = run_cmd(["jj", "commit", "-m", msg])
    return code == 0


def jj_revert(files_before: set[str] | None = None) -> None:
    """Revert changes made during fix attempt."""
    if files_before is not None:
        files_after = jj_get_modified_files()
        new_changes = files_after - files_before
        if new_changes:
            log("Reverting uncommitted changes...")
            for f in new_changes:
                run_cmd(["jj", "restore", f])
    else:
        if jj_get_modified_files():
            log("Reverting uncommitted changes...")
            run_cmd(["jj", "restore"])


def try_fix_file(failure: dict, make_output: str, timeout: int) -> bool:
    """
    Attempt to fix a data race.
    Returns True if fixed, False if exhausted attempts.
    """
    file = failure["file"]

    files_before = jj_get_modified_files()

    for attempt in range(1, MAX_ATTEMPTS_PER_FILE + 1):
        model, thinking_budget, model_name = ESCALATION_LADDER[attempt]

        log(f"Trying {model_name} (attempt {attempt}/{MAX_ATTEMPTS_PER_FILE})")

        prompt = build_prompt(failure, make_output)
        success, response = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

        if not success:
            log(f"{model_name} invocation FAILED: {response[:200]}")
            continue

        # Verify the fix
        check_passed, _ = run_make_tsan()

        if check_passed:
            log(f"{model_name} SUCCESS")
            jj_commit(failure, model_name, attempt, files_before)
            return True
        else:
            log(f"{model_name} FAILED (data race still present)")

    # Exhausted all attempts
    log(f"SKIPPED - exhausted {MAX_ATTEMPTS_PER_FILE} attempts")
    jj_revert(files_before)
    return False


def main() -> int:
    """Main entry point."""
    start_time = time.time()
    parser = argparse.ArgumentParser(description="Automated ThreadSanitizer error fix loop")
    parser.add_argument("--dry-run", action="store_true",
                        help="Identify data races without fixing them")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    args = parser.parse_args()

    opts = []
    if args.dry_run:
        opts.append("dry-run")
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")
    log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Clean up test databases before running
    drop_test_databases()

    # Run tsan check
    success, output = run_make_tsan()

    if success:
        log(f"All ThreadSanitizer checks passed! (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    failures = parse_failures(output)

    if not failures:
        log("make check-tsan failed but couldn't parse failures")
        log("Output tail:")
        for line in output.split('\n')[-30:]:
            log(f"  {line}")
        log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
        return 1

    log(f"Found {len(failures)} data races")

    # Dry-run: just list failures and exit
    if args.dry_run:
        log("Errors needing fixes:")
        for failure in failures:
            log(f"  - {failure['file']}:{failure['line']} - {failure['error_type']}")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    pass_num = 0

    while True:
        pass_num += 1
        log(f"=== Pass {pass_num} ===")

        if pass_num > 1:
            # Re-run check for subsequent passes
            success, output = run_make_tsan()

            if success:
                log(f"All ThreadSanitizer checks passed! (elapsed: {format_elapsed(time.time() - start_time)})")
                return 0

            failures = parse_failures(output)

            if not failures:
                log("make check-tsan failed but couldn't parse failures")
                log("Output tail:")
                for line in output.split('\n')[-30:]:
                    log(f"  {line}")
                log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
                return 1

            log(f"Found {len(failures)} data races")

        fixed_count = 0
        skipped = []
        file_times: list[float] = []

        for i, failure in enumerate(failures, 1):
            file = failure["file"]
            error = failure["error_type"]
            log(f"[file {i}/{len(failures)}] {file}:{failure['line']} - {error}")

            file_start = time.time()
            if try_fix_file(failure, output, args.time_out):
                fixed_count += 1
            else:
                skipped.append(f"{file}:{failure['line']}")
            file_elapsed = time.time() - file_start
            file_times.append(file_elapsed)
            avg_time = sum(file_times) / len(file_times)
            remaining = len(failures) - i
            eta = avg_time * remaining
            log(f"elapsed: {format_elapsed(file_elapsed)} | ETA: {format_elapsed(eta)}")

        log(f"Pass {pass_num} complete: {fixed_count} fixed, {len(skipped)} skipped")

        if fixed_count > 0 and len(skipped) == 0:
            log(f"All failures fixed! (elapsed: {format_elapsed(time.time() - start_time)})")
            return 0

        if fixed_count == 0:
            log("No progress made. Stopping.")
            if skipped:
                log("Skipped errors:")
                for f in skipped:
                    log(f"  - {f}")
            log(f"Completed with failures (elapsed: {format_elapsed(time.time() - start_time)})")
            return 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
