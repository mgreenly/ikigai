#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/prune - Automated dead code removal loop

Runs scripts/dead-code.sh, removes functions one at a time with escalation,
commits on success, records false positives, continues until all processed
or no progress is made.
"""

import argparse
import subprocess
import sys
import os
import json
import threading
import time
from datetime import datetime
from pathlib import Path

# Configuration
MAX_TEST_FIX_ATTEMPTS = 3

# Phase 1: Remove function (mechanical - use Haiku)
REMOVE_MODEL = ("haiku", "0", "haiku")

# Phase 2: Fix tests (escalation ladder)
TEST_FIX_LADDER = {
    1: ("sonnet", "10000", "sonnet:think"),
    2: ("opus", "10000", "opus:think"),
    3: ("opus", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
REMOVE_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
DELETE_TESTS_PROMPT_TEMPLATE = SCRIPT_DIR / "delete-tests.prompt.md"
REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE = SCRIPT_DIR / "refactor-remove-call.prompt.md"
JUSTIFY_HELPER_PROMPT_TEMPLATE = SCRIPT_DIR / "justify-helper.prompt.md"
CREATE_HELPER_PROMPT_TEMPLATE = SCRIPT_DIR / "create-helper.prompt.md"
REFACTOR_TEST_PROMPT_TEMPLATE = SCRIPT_DIR / "refactor-test.prompt.md"
FALSE_POSITIVES_FILE = PROJECT_ROOT / ".claude" / "data" / "dead-code-false-positives.txt"
TEST_HELPERS_DIR = PROJECT_ROOT / "tests" / "helpers"
DEFAULT_TIMEOUT = 600


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
    disabled = False  # Class-level flag to disable all spinners

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        if Spinner.disabled:
            return
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'prune':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def git_is_clean() -> bool:
    """Check if git workspace is clean."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    return code == 0 and not stdout.strip()


def git_get_modified_files() -> set[str]:
    """Get set of currently modified/untracked files."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    files = set()
    for line in stdout.split('\n'):
        if len(line) >= 4:  # XY + space + at least 1 char filename
            # Format: XY PATH or XY ORIG -> PATH (for renames)
            parts = line[3:].split(' -> ')
            files.add(parts[-1])
    return files


def git_commit(function: str, model_name: str) -> bool:
    """Commit all current changes."""
    if git_is_clean():
        return False

    run_cmd(["git", "add", "-A"])
    msg = f"refactor: remove dead code {function}\n\nharness/prune | {model_name}"

    code, _, _ = run_cmd(["git", "commit", "-m", msg])
    return code == 0


def git_revert() -> None:
    """Revert all uncommitted changes (tracked and untracked)."""
    if not git_is_clean():
        log("Reverting uncommitted changes...")
        run_cmd(["git", "reset", "--hard"])
        run_cmd(["git", "clean", "-fd"])


def get_dead_code_candidates() -> list[dict]:
    """
    Run scripts/dead-code.sh and parse output.
    Returns list of {function, file, line} dicts.
    """
    log("Running dead-code analysis...")
    spinner = Spinner("Analyzing dead code")
    spinner.start()
    code, stdout, stderr = run_cmd(["./scripts/dead-code.sh"])
    spinner.stop()

    if code != 0:
        log(f"ERROR: dead-code.sh failed: {stderr}")
        return []

    candidates = []
    for line in stdout.strip().split('\n'):
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith('#'):
            if "No orphaned functions" in line:
                return []
            continue

        # Format: function:file:line
        parts = line.split(':')
        if len(parts) >= 3:
            candidates.append({
                "function": parts[0],
                "file": parts[1],
                "line": parts[2],
            })

    return candidates


def build_prompt(template_path: Path, candidate: dict) -> str:
    """Build a prompt from template."""
    if not template_path.exists():
        log(f"ERROR: Missing {template_path}")
        sys.exit(1)

    template = template_path.read_text()
    prompt = template.replace("{{function}}", candidate["function"])
    prompt = template.replace("{{file}}", candidate["file"])
    prompt = prompt.replace("{{line}}", candidate["line"])
    return prompt


def build_delete_tests_prompt(function: str, test_file: str, tests_to_delete: list[str]) -> str:
    """Build prompt to delete specific tests from a file."""
    if not DELETE_TESTS_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {DELETE_TESTS_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = DELETE_TESTS_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{tests_to_delete}}", "\n".join(f"- {t}" for t in tests_to_delete))
    return prompt


def build_refactor_remove_call_prompt(function: str, test_file: str, tests_to_refactor: list[str]) -> str:
    """Build prompt to refactor tests to not call the function."""
    if not REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{tests_to_refactor}}", "\n".join(f"- {t}" for t in tests_to_refactor))
    return prompt


def build_justify_helper_prompt(function: str, test_file: str) -> str:
    """Build prompt to justify whether a helper should be created."""
    if not JUSTIFY_HELPER_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {JUSTIFY_HELPER_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = JUSTIFY_HELPER_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    return prompt


def build_create_helper_prompt(function: str, source_file: str) -> str:
    """Build prompt to create a test helper from removed function."""
    if not CREATE_HELPER_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {CREATE_HELPER_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = CREATE_HELPER_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{source_file}}", source_file)
    prompt = prompt.replace("{{helpers_dir}}", str(TEST_HELPERS_DIR))
    return prompt


def build_refactor_test_prompt(function: str, test_file: str, helper_header: str) -> str:
    """Build prompt to refactor test to use helper instead of removed function."""
    if not REFACTOR_TEST_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {REFACTOR_TEST_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = REFACTOR_TEST_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = template.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{helper_header}}", helper_header)
    return prompt


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, error_message).
    """
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = [
            "claude", "-p", "-",
            "--model", model,
            "--allowedTools", "Read,Edit,Write,Bash,Glob,Grep",
            "--max-turns", "20",
        ]
        env = os.environ.copy()
        if thinking_budget and thinking_budget != "0":
            env["MAX_THINKING_TOKENS"] = thinking_budget

        with open(prompt_file, 'r') as pf:
            spinner.start()
            result = subprocess.run(
                cmd,
                stdin=pf,
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=timeout,
                env=env,
            )
            spinner.stop()

        if result.returncode != 0:
            error = result.stderr.strip() if result.stderr else "unknown error"
            return False, error[:200]
        return True, ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"timeout after {timeout}s"
    finally:
        Path(prompt_file).unlink(missing_ok=True)


def invoke_claude_with_output(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str, str]:
    """
    Invoke Claude CLI and capture output.
    Returns (success, output, error_message).
    """
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = [
            "claude", "-p", "-",
            "--model", model,
            "--allowedTools", "Read",  # Read-only for analysis
            "--max-turns", "5",
        ]
        env = os.environ.copy()
        if thinking_budget and thinking_budget != "0":
            env["MAX_THINKING_TOKENS"] = thinking_budget

        with open(prompt_file, 'r') as pf:
            spinner.start()
            result = subprocess.run(
                cmd,
                stdin=pf,
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=timeout,
                env=env,
            )
            spinner.stop()

        if result.returncode != 0:
            error = result.stderr.strip() if result.stderr else "unknown error"
            return False, "", error[:200]
        return True, result.stdout, ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, "", f"timeout after {timeout}s"
    finally:
        Path(prompt_file).unlink(missing_ok=True)


def parse_justify_decision(output: str) -> tuple[str, str]:
    """
    Parse the justify-helper output.
    Returns (decision, reason) where decision is 'APPROVE', 'REJECT', or 'UNKNOWN'.
    """
    import re

    # Look for DECISION: APPROVE or DECISION: REJECT
    decision_match = re.search(r'DECISION:\s*(APPROVE|REJECT)', output, re.IGNORECASE)
    reason_match = re.search(r'REASON:\s*(.+?)(?:\n|$)', output, re.IGNORECASE)

    decision = decision_match.group(1).upper() if decision_match else "UNKNOWN"
    reason = reason_match.group(1).strip() if reason_match else "No reason provided"

    return decision, reason


def record_false_positive(function: str) -> None:
    """Record a false positive to the exclusion file."""
    FALSE_POSITIVES_FILE.parent.mkdir(parents=True, exist_ok=True)

    existing = set()
    if FALSE_POSITIVES_FILE.exists():
        existing = set(FALSE_POSITIVES_FILE.read_text().strip().split('\n'))

    if function not in existing:
        with open(FALSE_POSITIVES_FILE, 'a') as f:
            f.write(f"{function}\n")
        log(f"Recorded false positive: {function}")


def run_build() -> bool:
    """Run make bin/ikigai and return success."""
    log("Verifying build...")
    spinner = Spinner("Building")
    spinner.start()
    code, _, _ = run_cmd(["make", "bin/ikigai"])
    spinner.stop()
    if code == 0:
        log("Build: PASS")
    else:
        log("Build: FAIL")
    return code == 0


def finalize_removal(function: str, modified_files: set[str]) -> None:
    """Delete #if 0 blocks that were used to comment out the function."""
    import re

    marker = f"#if 0  // dead code: {function}"

    for file_path in modified_files:
        path = Path(file_path)
        if not path.exists():
            continue

        content = path.read_text()
        if marker not in content:
            continue

        # Remove #if 0 ... #endif blocks with our marker
        # Pattern: #if 0  // dead code: func_name ... #endif
        lines = content.split('\n')
        new_lines = []
        in_block = False

        for line in lines:
            if marker in line:
                in_block = True
                continue
            if in_block and line.strip() == '#endif':
                in_block = False
                continue
            if not in_block:
                new_lines.append(line)

        path.write_text('\n'.join(new_lines))
        log(f"Finalized: {file_path}")


def find_tests_using_function(function: str) -> list[str]:
    """Find all test files that reference the function using grep."""
    log(f"Searching for tests using {function}...")
    spinner = Spinner("Searching")
    spinner.start()

    # Grep for function calls in test files
    code, stdout, _ = run_cmd([
        "grep", "-rl",
        f"\\b{function}\\s*(",
        "tests/"
    ])
    spinner.stop()

    if code != 0 or not stdout.strip():
        log("No tests use this function")
        return []

    # Filter to only .c files
    result = sorted([f for f in stdout.strip().split('\n') if f.endswith('.c')])
    if result:
        log(f"Tests using function: {', '.join(result)}")
    return result


def analyze_test_file(test_file: str, function: str) -> tuple[int, int, list[str]]:
    """
    Analyze a test file to count tests using the function.
    Returns (tests_using_function, total_tests, test_names_using_function).
    """
    import re

    path = Path(test_file)
    if not path.exists():
        return 0, 0, []

    content = path.read_text()

    # Find all START_TEST(test_name) declarations
    test_pattern = re.compile(r'START_TEST\s*\(\s*(\w+)\s*\)')
    all_tests = test_pattern.findall(content)
    total_tests = len(all_tests)

    # Find tests that use the function
    # Parse each test block and check if it contains the function call
    tests_using = []
    for match in test_pattern.finditer(content):
        test_name = match.group(1)
        start_pos = match.start()

        # Find the END_TEST for this test
        end_match = re.search(r'\bEND_TEST\b', content[start_pos:])
        if end_match:
            test_block = content[start_pos:start_pos + end_match.end()]
            # Check if this test block contains the function
            if re.search(rf'\b{re.escape(function)}\s*\(', test_block):
                tests_using.append(test_name)

    return len(tests_using), total_tests, tests_using


def delete_test_file(test_file: str) -> bool:
    """Delete a test file. Returns True if successful."""
    path = Path(test_file)
    if path.exists():
        path.unlink()
        log(f"Deleted: {test_file}")
        return True
    return False


def build_single_test(test_file: str) -> bool:
    """Build a single test file. Returns True if successful."""
    spinner = Spinner(f"Building {Path(test_file).name}")
    spinner.start()
    code, _, _ = run_cmd(["make", f"bin/{test_file.replace('.c', '')}"])
    spinner.stop()
    return code == 0


def run_single_test(test_file: str) -> bool:
    """Run a single test file. Returns True if successful."""
    spinner = Spinner(f"Testing {Path(test_file).name}")
    spinner.start()
    code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_file}"])
    spinner.stop()
    return code == 0


def run_tests(test_files: list[str] | None = None) -> bool:
    """Run make check, optionally for specific test files."""
    if test_files:
        # Run only specified tests
        test_list = " ".join(test_files)
        log(f"Verifying tests: {test_list}")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_list}"])
        spinner.stop()
    else:
        # Run all tests
        log("Verifying all tests...")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check"])
        spinner.stop()

    if code == 0:
        log("Tests: PASS")
    else:
        log("Tests: FAIL")
    return code == 0


def try_remove_function(candidate: dict, timeout: int) -> tuple[bool, str]:
    """
    Attempt to remove a dead code function.
    Returns (success, reason).
    """
    function = candidate["function"]
    file = candidate["file"]

    # Defensive: ensure clean workspace before starting
    if not git_is_clean():
        log("WARNING: Dirty workspace detected - cleaning before attempt")
        git_revert()
        if not git_is_clean():
            log("ERROR: Could not clean workspace")
            return False, "dirty workspace"

    # Phase 1: Remove the function (Haiku - mechanical task)
    model, thinking_budget, model_name = REMOVE_MODEL
    log(f"Phase 1: Remove function ({model_name})")

    prompt = build_prompt(REMOVE_PROMPT_TEMPLATE, candidate)
    success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)
    if not success:
        log(f"{model_name} invocation FAILED: {error}")
        git_revert()
        return False, "removal failed"

    # Verify changes were made
    if git_is_clean():
        log(f"Agent made no changes")
        return False, "no changes made"

    log(f"Files changed: {', '.join(sorted(git_get_modified_files()))}")

    # Check build (production code only)
    if not run_build():
        log(f"False positive - function is used in production")
        git_revert()
        record_false_positive(function)
        return False, "false positive"

    # Find tests that use the removed function
    broken_tests = find_tests_using_function(function)
    if not broken_tests:
        # No tests broken - finalize and commit
        log("Finalizing: deleting commented code")
        finalize_removal(function, git_get_modified_files())
        log(f"SUCCESS - removed {function}")
        git_commit(function, model_name)
        return True, "removed"

    # Phase 2: Fix broken tests (one file at a time)
    log(f"Phase 2: Fixing {len(broken_tests)} broken test file(s)")

    all_tests_fixed = True
    test_files_deleted = []
    test_files_modified = []
    final_model_name = model_name  # Track which model succeeded

    for test_file in broken_tests:
        log(f"Analyzing: {test_file}")

        # Analyze the test file deterministically
        tests_using, total_tests, test_names = analyze_test_file(test_file, function)
        log(f"  {tests_using}/{total_tests} tests use {function}")

        if tests_using == 0:
            # File doesn't actually use the function - might be indirect dependency
            log(f"  No direct usage found - skipping")
            continue

        if tests_using == total_tests:
            # ALL tests use the function - delete entire file (no LLM needed)
            log(f"  All tests affected - deleting file")
            delete_test_file(test_file)
            test_files_deleted.append(test_file)
            continue

        # Partial: some tests use function, others don't
        # Try deletion first, then refactor, then justify helper
        log(f"  Partial ({tests_using}/{total_tests}) - trying deletion first")

        file_fixed = False

        # Step 1: Try deleting just the tests that use the function
        log(f"  Step 1: Delete tests that use {function}")
        for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
            model, thinking_budget, model_name = TEST_FIX_LADDER[attempt]
            log(f"    Attempt {attempt}/{MAX_TEST_FIX_ATTEMPTS} ({model_name})")

            prompt = build_delete_tests_prompt(function, test_file, test_names)
            success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

            if not success:
                log(f"    LLM invocation failed: {error}")
                continue

            # Verify the test file still compiles and passes
            if build_single_test(test_file) and run_single_test(test_file):
                log(f"    Deleted tests from: {test_file}")
                test_files_modified.append(test_file)
                final_model_name = model_name
                file_fixed = True
                break

            log(f"    Still failing - reverting")
            git_revert()

        if file_fixed:
            continue

        # Step 2: Try refactoring tests to not call the function
        log(f"  Step 2: Refactor tests to remove {function} calls")
        for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
            model, thinking_budget, model_name = TEST_FIX_LADDER[attempt]
            log(f"    Attempt {attempt}/{MAX_TEST_FIX_ATTEMPTS} ({model_name})")

            prompt = build_refactor_remove_call_prompt(function, test_file, test_names)
            success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

            if not success:
                log(f"    LLM invocation failed: {error}")
                continue

            # Verify the test file still compiles and passes
            if build_single_test(test_file) and run_single_test(test_file):
                log(f"    Refactored: {test_file}")
                test_files_modified.append(test_file)
                final_model_name = model_name
                file_fixed = True
                break

            log(f"    Still failing - reverting")
            git_revert()

        if file_fixed:
            continue

        # Step 3: Ask for justification before creating helper
        log(f"  Step 3: Checking if helper is justified")
        model, thinking_budget, model_name = TEST_FIX_LADDER[1]  # Use sonnet:think for analysis
        prompt = build_justify_helper_prompt(function, test_file)
        success, output, error = invoke_claude_with_output(prompt, model, thinking_budget, model_name, timeout)

        if not success:
            log(f"    LLM invocation failed: {error}")
            log(f"  SKIPPED - could not determine justification")
            all_tests_fixed = False
            break

        decision, reason = parse_justify_decision(output)
        log(f"    Decision: {decision}")
        log(f"    Reason: {reason}")

        if decision != "APPROVE":
            # No justification for helper - skip this candidate
            log(f"  SKIPPED - no justification for helper (tests should be deleted)")
            all_tests_fixed = False
            break

        # Step 4: Helper is justified - create it and refactor
        log(f"  Step 4: Creating justified helper")
        helper_header = f"tests/helpers/{function}.h"

        if not Path(helper_header).exists():
            for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
                model, thinking_budget, model_name = TEST_FIX_LADDER[attempt]
                log(f"    Creating helper attempt {attempt}/{MAX_TEST_FIX_ATTEMPTS} ({model_name})")

                prompt = build_create_helper_prompt(function, file)
                success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

                if not success:
                    log(f"    LLM invocation failed: {error}")
                    continue

                if Path(helper_header).exists():
                    log(f"    Created: {helper_header}")
                    break

                log(f"    Helper not created - reverting")
                git_revert()
            else:
                log(f"  FAILED: Could not create helper")
                all_tests_fixed = False
                break

        # Refactor the test to use the helper
        for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
            model, thinking_budget, model_name = TEST_FIX_LADDER[attempt]
            log(f"    Refactoring to use helper attempt {attempt}/{MAX_TEST_FIX_ATTEMPTS} ({model_name})")

            prompt = build_refactor_test_prompt(function, test_file, helper_header)
            success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

            if not success:
                log(f"    LLM invocation failed: {error}")
                continue

            # Verify this specific test file compiles and passes
            if build_single_test(test_file) and run_single_test(test_file):
                log(f"    Refactored: {test_file}")
                test_files_modified.append(test_file)
                final_model_name = model_name
                file_fixed = True
                break

            log(f"    Still failing - reverting")
            git_revert()

        if not file_fixed:
            log(f"  FAILED: Could not refactor {test_file}")
            all_tests_fixed = False
            break

    if not all_tests_fixed:
        log(f"SKIPPED - could not fix all test files")
        git_revert()
        return False, "test fixes exhausted"

    # Verify all tests pass together
    remaining_tests = [t for t in broken_tests if t not in test_files_deleted]
    if remaining_tests:
        log(f"Final verification: {len(remaining_tests)} test file(s)")
        if not run_tests(remaining_tests):
            log(f"SKIPPED - final verification failed")
            git_revert()
            return False, "final verification failed"

    # All tests fixed - finalize and commit
    log("Finalizing: deleting commented code")
    finalize_removal(function, git_get_modified_files())

    summary_parts = []
    if test_files_deleted:
        summary_parts.append(f"deleted {len(test_files_deleted)} test file(s)")
    if test_files_modified:
        summary_parts.append(f"modified {len(test_files_modified)} test file(s)")
    summary = ", ".join(summary_parts) if summary_parts else "no test changes needed"

    log(f"SUCCESS - removed {function} ({summary})")
    git_commit(function, final_model_name)
    return True, "removed with test fixes"


def main() -> int:
    """Main entry point."""
    start_time = time.time()

    parser = argparse.ArgumentParser(description="Automated dead code removal loop")
    parser.add_argument("--dry-run", action="store_true",
                        help="Identify dead code without removing it")
    parser.add_argument("--function", type=str, default=None,
                        help="Only process this specific function")
    parser.add_argument("--no-spinner", action="store_true",
                        help="Disable spinner animation (useful for non-interactive output)")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    args = parser.parse_args()

    if args.no_spinner:
        Spinner.disabled = True

    opts = []
    if args.dry_run:
        opts.append("dry-run")
    if args.function:
        opts.append(f"function={args.function}")
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")
    log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Precondition: git must be clean
    if not git_is_clean():
        log("ERROR: Git workspace must be clean before pruning.")
        return 1

    # Get candidates
    candidates = get_dead_code_candidates()

    # Filter to specific function if requested
    if args.function:
        candidates = [c for c in candidates if c["function"] == args.function]
        if not candidates:
            log(f"Function '{args.function}' not found in dead code candidates")
            return 1

    if not candidates:
        log("No dead code found.")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    log(f"Found {len(candidates)} dead code candidates")

    # Dry-run: just list candidates and exit
    if args.dry_run:
        log("Dead code candidates:")
        for c in candidates:
            log(f"  - {c['function']} at {c['file']}:{c['line']}")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    removed = 0
    skipped = []
    fix_times: list[float] = []

    for i, candidate in enumerate(candidates, 1):
        function = candidate["function"]
        file = candidate["file"]
        line = candidate["line"]

        log(f"[{i}/{len(candidates)}] {function} at {file}:{line}")

        fix_start = time.time()
        success, reason = try_remove_function(candidate, args.time_out)
        fix_elapsed = time.time() - fix_start
        fix_times.append(fix_elapsed)

        if success:
            removed += 1
        else:
            skipped.append(f"{function} - {reason}")

        avg_time = sum(fix_times) / len(fix_times)
        remaining = len(candidates) - i
        eta = avg_time * remaining
        log(f"elapsed: {format_elapsed(fix_elapsed)} | ETA: {format_elapsed(eta)}")

    # Summary
    log("")
    log("/prune complete")
    log("")
    log(f"Removed: {removed}")
    log(f"Skipped: {len(skipped)}")

    if skipped:
        log("")
        log("Skipped functions:")
        for s in skipped:
            log(f"  - {s}")

    log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")

    return 0 if removed > 0 or len(skipped) == 0 else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
