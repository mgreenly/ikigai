#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/prune - Automated dead code removal loop

Runs scripts/dead-code.sh, removes functions one at a time with escalation,
commits on success, records false positives, continues until all processed
or no progress is made.
"""

import argparse
import subprocess
import sys
import os
import json
import threading
import time
from datetime import datetime
from pathlib import Path

# Configuration
MAX_TEST_FIX_ATTEMPTS = 3

# Phase 1: Remove function (mechanical - use Haiku)
REMOVE_MODEL = ("haiku", "0", "haiku")

# Phase 2: Fix tests (escalation ladder)
TEST_FIX_LADDER = {
    1: ("sonnet", "10000", "sonnet:think"),
    2: ("opus", "10000", "opus:think"),
    3: ("opus", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
REMOVE_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
FIX_TESTS_PROMPT_TEMPLATE = SCRIPT_DIR / "fix-tests.prompt.md"
FALSE_POSITIVES_FILE = PROJECT_ROOT / ".claude" / "data" / "dead-code-false-positives.txt"
DEFAULT_TIMEOUT = 600


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
    disabled = False  # Class-level flag to disable all spinners

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        if Spinner.disabled:
            return
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'prune':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def git_is_clean() -> bool:
    """Check if git workspace is clean."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    return code == 0 and not stdout.strip()


def git_get_modified_files() -> set[str]:
    """Get set of currently modified/untracked files."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    files = set()
    for line in stdout.split('\n'):
        if len(line) >= 4:  # XY + space + at least 1 char filename
            # Format: XY PATH or XY ORIG -> PATH (for renames)
            parts = line[3:].split(' -> ')
            files.add(parts[-1])
    return files


def git_commit(function: str, file: str, model_name: str, attempt: int, files_before: set[str]) -> bool:
    """Commit only files that changed during fix attempt."""
    files_after = git_get_modified_files()
    new_changes = files_after - files_before

    if not new_changes:
        return False

    for f in new_changes:
        run_cmd(["git", "add", f])

    msg = f"refactor: remove dead code {function}\n\nharness/prune | {model_name} | attempt {attempt}"

    code, _, _ = run_cmd(["git", "commit", "-m", msg])
    return code == 0


def git_revert(files_before: set[str] | None = None) -> None:
    """Revert changes made during fix attempt."""
    if files_before is not None:
        files_after = git_get_modified_files()
        new_changes = files_after - files_before
        if new_changes:
            log("Reverting uncommitted changes...")
            for f in new_changes:
                run_cmd(["git", "checkout", f])
    else:
        code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
        if stdout.strip():
            log("Reverting uncommitted changes...")
            run_cmd(["git", "checkout", "."])
            run_cmd(["git", "clean", "-fd"])


def get_dead_code_candidates() -> list[dict]:
    """
    Run scripts/dead-code.sh and parse output.
    Returns list of {function, file, line} dicts.
    """
    log("Running dead-code analysis...")
    spinner = Spinner("Analyzing dead code")
    spinner.start()
    code, stdout, stderr = run_cmd(["./scripts/dead-code.sh"])
    spinner.stop()

    if code != 0:
        log(f"ERROR: dead-code.sh failed: {stderr}")
        return []

    candidates = []
    for line in stdout.strip().split('\n'):
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith('#'):
            if "No orphaned functions" in line:
                return []
            continue

        # Format: function:file:line
        parts = line.split(':')
        if len(parts) >= 3:
            candidates.append({
                "function": parts[0],
                "file": parts[1],
                "line": parts[2],
            })

    return candidates


def build_prompt(template_path: Path, candidate: dict) -> str:
    """Build a prompt from template."""
    if not template_path.exists():
        log(f"ERROR: Missing {template_path}")
        sys.exit(1)

    template = template_path.read_text()
    prompt = template.replace("{{function}}", candidate["function"])
    prompt = template.replace("{{file}}", candidate["file"])
    prompt = prompt.replace("{{line}}", candidate["line"])
    return prompt


def build_fix_tests_prompt(function: str, test_file: str, tests_to_delete: list[str]) -> str:
    """Build fix-tests prompt for a single test file."""
    if not FIX_TESTS_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {FIX_TESTS_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = FIX_TESTS_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{tests_to_delete}}", "\n".join(f"- {t}" for t in tests_to_delete))
    return prompt


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, error_message).
    """
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = [
            "claude", "-p", "-",
            "--model", model,
            "--allowedTools", "Read,Edit,Write,Bash,Glob,Grep",
            "--max-turns", "20",
        ]
        env = os.environ.copy()
        if thinking_budget and thinking_budget != "0":
            env["MAX_THINKING_TOKENS"] = thinking_budget

        with open(prompt_file, 'r') as pf:
            spinner.start()
            result = subprocess.run(
                cmd,
                stdin=pf,
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=timeout,
                env=env,
            )
            spinner.stop()

        if result.returncode != 0:
            error = result.stderr.strip() if result.stderr else "unknown error"
            return False, error[:200]
        return True, ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"timeout after {timeout}s"
    finally:
        Path(prompt_file).unlink(missing_ok=True)


def record_false_positive(function: str) -> None:
    """Record a false positive to the exclusion file."""
    FALSE_POSITIVES_FILE.parent.mkdir(parents=True, exist_ok=True)

    existing = set()
    if FALSE_POSITIVES_FILE.exists():
        existing = set(FALSE_POSITIVES_FILE.read_text().strip().split('\n'))

    if function not in existing:
        with open(FALSE_POSITIVES_FILE, 'a') as f:
            f.write(f"{function}\n")
        log(f"Recorded false positive: {function}")


def run_build() -> bool:
    """Run make bin/ikigai and return success."""
    log("Verifying build...")
    spinner = Spinner("Building")
    spinner.start()
    code, _, _ = run_cmd(["make", "bin/ikigai"])
    spinner.stop()
    if code == 0:
        log("Build: PASS")
    else:
        log("Build: FAIL")
    return code == 0


def finalize_removal(function: str, modified_files: set[str]) -> None:
    """Delete #if 0 blocks that were used to comment out the function."""
    import re

    marker = f"#if 0  // dead code: {function}"

    for file_path in modified_files:
        path = Path(file_path)
        if not path.exists():
            continue

        content = path.read_text()
        if marker not in content:
            continue

        # Remove #if 0 ... #endif blocks with our marker
        # Pattern: #if 0  // dead code: func_name ... #endif
        lines = content.split('\n')
        new_lines = []
        in_block = False

        for line in lines:
            if marker in line:
                in_block = True
                continue
            if in_block and line.strip() == '#endif':
                in_block = False
                continue
            if not in_block:
                new_lines.append(line)

        path.write_text('\n'.join(new_lines))
        log(f"Finalized: {file_path}")


def find_broken_tests() -> list[str]:
    """Run make build-tests and find which test files fail to compile/link."""
    log("Finding broken tests...")
    spinner = Spinner("Compiling tests")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", "build-tests"])
    spinner.stop()

    if code == 0:
        log("All tests compile")
        return []

    # Parse compile/link errors to find affected test files
    # Compile error format: tests/unit/foo/bar_test.c:123:45: error: ...
    # Link error format: /full/path/tests/unit/foo/bar_test.c:123: undefined reference to ...
    broken = set()
    import re
    for line in (stdout + stderr).split('\n'):
        # Look for .c files in tests/ directory with error indicators
        if 'error:' in line or 'undefined reference' in line:
            # Extract test file path - could be relative or absolute
            match = re.search(r'(/?(?:[^/:]*/)*(tests/[^:]+\.c))', line)
            if match:
                test_file = match.group(2)  # Get just the tests/... part
                broken.add(test_file)

    result = sorted(broken)
    if result:
        log(f"Broken tests: {', '.join(result)}")
    return result


def analyze_test_file(test_file: str, function: str) -> tuple[int, int, list[str]]:
    """
    Analyze a test file to count tests using the function.
    Returns (tests_using_function, total_tests, test_names_using_function).
    """
    import re

    path = Path(test_file)
    if not path.exists():
        return 0, 0, []

    content = path.read_text()

    # Find all START_TEST(test_name) declarations
    test_pattern = re.compile(r'START_TEST\s*\(\s*(\w+)\s*\)')
    all_tests = test_pattern.findall(content)
    total_tests = len(all_tests)

    # Find tests that use the function
    # Parse each test block and check if it contains the function call
    tests_using = []
    for match in test_pattern.finditer(content):
        test_name = match.group(1)
        start_pos = match.start()

        # Find the END_TEST for this test
        end_match = re.search(r'\bEND_TEST\b', content[start_pos:])
        if end_match:
            test_block = content[start_pos:start_pos + end_match.end()]
            # Check if this test block contains the function
            if re.search(rf'\b{re.escape(function)}\s*\(', test_block):
                tests_using.append(test_name)

    return len(tests_using), total_tests, tests_using


def delete_test_file(test_file: str) -> bool:
    """Delete a test file. Returns True if successful."""
    path = Path(test_file)
    if path.exists():
        path.unlink()
        log(f"Deleted: {test_file}")
        return True
    return False


def build_single_test(test_file: str) -> bool:
    """Build a single test file. Returns True if successful."""
    spinner = Spinner(f"Building {Path(test_file).name}")
    spinner.start()
    code, _, _ = run_cmd(["make", f"bin/{test_file.replace('.c', '')}"])
    spinner.stop()
    return code == 0


def run_single_test(test_file: str) -> bool:
    """Run a single test file. Returns True if successful."""
    spinner = Spinner(f"Testing {Path(test_file).name}")
    spinner.start()
    code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_file}"])
    spinner.stop()
    return code == 0


def run_tests(test_files: list[str] | None = None) -> bool:
    """Run make check, optionally for specific test files."""
    if test_files:
        # Run only specified tests
        test_list = " ".join(test_files)
        log(f"Verifying tests: {test_list}")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_list}"])
        spinner.stop()
    else:
        # Run all tests
        log("Verifying all tests...")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check"])
        spinner.stop()

    if code == 0:
        log("Tests: PASS")
    else:
        log("Tests: FAIL")
    return code == 0


def try_remove_function(candidate: dict, timeout: int) -> tuple[bool, str]:
    """
    Attempt to remove a dead code function.
    Returns (success, reason).
    """
    function = candidate["function"]
    file = candidate["file"]

    files_before = git_get_modified_files()

    # Phase 1: Remove the function (Haiku - mechanical task)
    model, thinking_budget, model_name = REMOVE_MODEL
    log(f"Phase 1: Remove function ({model_name})")

    prompt = build_prompt(REMOVE_PROMPT_TEMPLATE, candidate)
    success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)
    if not success:
        log(f"{model_name} invocation FAILED: {error}")
        return False, "removal failed"

    # Verify changes were made
    files_after = git_get_modified_files()
    changed = files_after - files_before
    if not changed:
        log(f"Agent made no changes")
        return False, "no changes made"

    log(f"Files changed: {', '.join(sorted(changed))}")

    # Check build (production code only)
    if not run_build():
        log(f"False positive - function is used in production")
        git_revert(files_before)
        record_false_positive(function)
        return False, "false positive"

    # Find tests that fail to compile after function removal
    broken_tests = find_broken_tests()
    if not broken_tests:
        # No tests broken - finalize and commit
        log("Finalizing: deleting commented code")
        finalize_removal(function, changed)
        log(f"SUCCESS - removed {function}")
        git_commit(function, file, model_name, 1, files_before)
        return True, "removed"

    # Phase 2: Fix broken tests (one file at a time)
    log(f"Phase 2: Fixing {len(broken_tests)} broken test file(s)")

    all_tests_fixed = True
    test_files_deleted = []
    test_files_modified = []
    final_model_name = model_name  # Track which model succeeded

    for test_file in broken_tests:
        log(f"Analyzing: {test_file}")

        # Analyze the test file deterministically
        tests_using, total_tests, test_names = analyze_test_file(test_file, function)
        log(f"  {tests_using}/{total_tests} tests use {function}")

        if tests_using == 0:
            # File doesn't actually use the function - might be indirect dependency
            log(f"  No direct usage found - skipping")
            continue

        if tests_using == total_tests:
            # ALL tests use the function - delete entire file (no LLM needed)
            log(f"  All tests affected - deleting file")
            delete_test_file(test_file)
            test_files_deleted.append(test_file)
            continue

        # Partial: some tests use function, others don't - need LLM
        log(f"  Partial ({tests_using}/{total_tests}) - using LLM to remove specific tests")

        file_fixed = False
        files_before_fix = git_get_modified_files()

        for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
            model, thinking_budget, model_name = TEST_FIX_LADDER[attempt]
            log(f"  Attempt {attempt}/{MAX_TEST_FIX_ATTEMPTS} ({model_name})")

            # Build prompt for this specific file and tests
            prompt = build_fix_tests_prompt(function, test_file, test_names)
            success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

            if not success:
                log(f"  LLM invocation failed: {error}")
                continue

            # Verify this specific test file compiles and passes
            if build_single_test(test_file) and run_single_test(test_file):
                log(f"  Fixed: {test_file}")
                test_files_modified.append(test_file)
                final_model_name = model_name
                file_fixed = True
                break

            log(f"  Still failing - reverting")
            # Revert changes to this file only
            files_after_fix = git_get_modified_files()
            for f in files_after_fix - files_before_fix:
                run_cmd(["git", "checkout", f])

        if not file_fixed:
            log(f"  FAILED: Could not fix {test_file}")
            all_tests_fixed = False
            break

    if not all_tests_fixed:
        log(f"SKIPPED - could not fix all test files")
        git_revert(files_before)
        return False, "test fixes exhausted"

    # Verify all tests pass together
    remaining_tests = [t for t in broken_tests if t not in test_files_deleted]
    if remaining_tests:
        log(f"Final verification: {len(remaining_tests)} test file(s)")
        if not run_tests(remaining_tests):
            log(f"SKIPPED - final verification failed")
            git_revert(files_before)
            return False, "final verification failed"

    # All tests fixed - finalize and commit
    log("Finalizing: deleting commented code")
    finalize_removal(function, changed)

    summary_parts = []
    if test_files_deleted:
        summary_parts.append(f"deleted {len(test_files_deleted)} test file(s)")
    if test_files_modified:
        summary_parts.append(f"modified {len(test_files_modified)} test file(s)")
    summary = ", ".join(summary_parts) if summary_parts else "no test changes needed"

    log(f"SUCCESS - removed {function} ({summary})")
    git_commit(function, file, final_model_name, 1, files_before)
    return True, "removed with test fixes"


def main() -> int:
    """Main entry point."""
    start_time = time.time()

    parser = argparse.ArgumentParser(description="Automated dead code removal loop")
    parser.add_argument("--dry-run", action="store_true",
                        help="Identify dead code without removing it")
    parser.add_argument("--function", type=str, default=None,
                        help="Only process this specific function")
    parser.add_argument("--no-spinner", action="store_true",
                        help="Disable spinner animation (useful for non-interactive output)")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    args = parser.parse_args()

    if args.no_spinner:
        Spinner.disabled = True

    opts = []
    if args.dry_run:
        opts.append("dry-run")
    if args.function:
        opts.append(f"function={args.function}")
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")
    log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Precondition: git must be clean
    if not git_is_clean():
        log("ERROR: Git workspace must be clean before pruning.")
        return 1

    # Get candidates
    candidates = get_dead_code_candidates()

    # Filter to specific function if requested
    if args.function:
        candidates = [c for c in candidates if c["function"] == args.function]
        if not candidates:
            log(f"Function '{args.function}' not found in dead code candidates")
            return 1

    if not candidates:
        log("No dead code found.")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    log(f"Found {len(candidates)} dead code candidates")

    # Dry-run: just list candidates and exit
    if args.dry_run:
        log("Dead code candidates:")
        for c in candidates:
            log(f"  - {c['function']} at {c['file']}:{c['line']}")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    removed = 0
    skipped = []
    fix_times: list[float] = []

    for i, candidate in enumerate(candidates, 1):
        function = candidate["function"]
        file = candidate["file"]
        line = candidate["line"]

        log(f"[{i}/{len(candidates)}] {function} at {file}:{line}")

        fix_start = time.time()
        success, reason = try_remove_function(candidate, args.time_out)
        fix_elapsed = time.time() - fix_start
        fix_times.append(fix_elapsed)

        if success:
            removed += 1
        else:
            skipped.append(f"{function} - {reason}")

        avg_time = sum(fix_times) / len(fix_times)
        remaining = len(candidates) - i
        eta = avg_time * remaining
        log(f"elapsed: {format_elapsed(fix_elapsed)} | ETA: {format_elapsed(eta)}")

    # Summary
    log("")
    log("/prune complete")
    log("")
    log(f"Removed: {removed}")
    log(f"Skipped: {len(skipped)}")

    if skipped:
        log("")
        log("Skipped functions:")
        for s in skipped:
            log(f"  - {s}")

    log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")

    return 0 if removed > 0 or len(skipped) == 0 else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
