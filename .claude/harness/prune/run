#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/prune - Automated dead code removal loop

Runs .claude/scripts/dead-code.sh, removes functions one at a time with escalation,
commits on success, records false positives, continues until all processed
or no progress is made.
"""

import argparse
import subprocess
import sys
import os
import json
import threading
import time
from datetime import datetime
from pathlib import Path

# Configuration
MAX_TEST_FIX_ATTEMPTS = 3

# Phase 1: Remove function (mechanical - use Haiku)
REMOVE_MODEL = ("haiku", "0", "haiku")

# Classification escalation ladder (starts with haiku, escalates on failure)
CLASSIFY_LADDER = {
    1: ("haiku", "0", "haiku"),
    2: ("sonnet", "10000", "sonnet:think"),
    3: ("opus", "10000", "opus:think"),
    4: ("opus", "128000", "opus:ultrathink"),
}

# Phase 2: Fix tests (escalation ladder)
TEST_FIX_LADDER = {
    1: ("sonnet", "10000", "sonnet:think"),
    2: ("opus", "10000", "opus:think"),
    3: ("opus", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
COMMENT_OUT_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
ANALYZE_SINGLE_TEST_PROMPT_TEMPLATE = SCRIPT_DIR / "analyze-single-test.prompt.md"
DELETE_TEST_PROMPT_TEMPLATE = SCRIPT_DIR / "delete-test.prompt.md"
DELETE_FUNCTION_PROMPT_TEMPLATE = SCRIPT_DIR / "delete-function.prompt.md"
CLEANUP_EMPTY_TCASE_PROMPT_TEMPLATE = SCRIPT_DIR / "cleanup-empty-tcase.prompt.md"
CLEANUP_UNUSED_STATICS_PROMPT_TEMPLATE = SCRIPT_DIR / "cleanup-unused-statics.prompt.md"
REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE = SCRIPT_DIR / "refactor-remove-call.prompt.md"
JUSTIFY_HELPER_PROMPT_TEMPLATE = SCRIPT_DIR / "justify-helper.prompt.md"
CREATE_HELPER_PROMPT_TEMPLATE = SCRIPT_DIR / "create-helper.prompt.md"
REFACTOR_TEST_PROMPT_TEMPLATE = SCRIPT_DIR / "refactor-test.prompt.md"
FALSE_POSITIVES_FILE = PROJECT_ROOT / ".claude" / "data" / "dead-code-false-positives.txt"
TEST_HELPERS_DIR = PROJECT_ROOT / "tests" / "helpers"
DEFAULT_TIMEOUT = 600


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
    disabled = False  # Class-level flag to disable all spinners

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        if Spinner.disabled:
            return
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'prune':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def git_is_clean() -> bool:
    """Check if git workspace is clean."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    return code == 0 and not stdout.strip()


def git_get_modified_files() -> set[str]:
    """Get set of currently modified/untracked files."""
    code, stdout, _ = run_cmd(["git", "status", "--porcelain"])
    files = set()
    for line in stdout.split('\n'):
        if len(line) >= 4:  # XY + space + at least 1 char filename
            # Format: XY PATH or XY ORIG -> PATH (for renames)
            parts = line[3:].split(' -> ')
            files.add(parts[-1])
    return files


def git_commit(function: str, model_name: str) -> bool:
    """Commit all current changes."""
    if git_is_clean():
        return False

    run_cmd(["git", "add", "-A"])
    msg = f"refactor: remove dead code {function}\n\nharness/prune | {model_name}"

    code, _, _ = run_cmd(["git", "commit", "-m", msg])
    return code == 0


def git_revert() -> None:
    """Revert all uncommitted changes (tracked and untracked)."""
    if not git_is_clean():
        log("Reverting uncommitted changes...")
        run_cmd(["git", "reset", "--hard"])
        run_cmd(["git", "clean", "-fd"])


def get_dead_code_candidates() -> list[dict]:
    """
    Run .claude/scripts/dead-code.sh and parse output.
    Returns list of {function, file, line} dicts.
    """
    log("Running dead-code analysis...")
    spinner = Spinner("Analyzing dead code")
    spinner.start()
    code, stdout, stderr = run_cmd(["./.claude/scripts/dead-code.sh"])
    spinner.stop()

    if code != 0:
        log(f"ERROR: dead-code.sh failed: {stderr}")
        return []

    candidates = []
    for line in stdout.strip().split('\n'):
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith('#'):
            if "No orphaned functions" in line:
                return []
            continue

        # Format: function:file:line
        parts = line.split(':')
        if len(parts) >= 3:
            candidates.append({
                "function": parts[0],
                "file": parts[1],
                "line": parts[2],
            })

    return candidates


def build_comment_out_prompt(candidate: dict) -> str:
    """Build prompt to comment out a function with #if 0."""
    if not COMMENT_OUT_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {COMMENT_OUT_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = COMMENT_OUT_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", candidate["function"])
    prompt = prompt.replace("{{file}}", candidate["file"])
    prompt = prompt.replace("{{line}}", candidate["line"])
    return prompt


def build_analyze_single_test_prompt(function: str, test_file: str, test_name: str) -> str:
    """Build prompt to analyze a single test (TESTING vs USING)."""
    if not ANALYZE_SINGLE_TEST_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {ANALYZE_SINGLE_TEST_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = ANALYZE_SINGLE_TEST_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{test_name}}", test_name)
    return prompt


def build_delete_function_prompt(candidate: dict) -> str:
    """Build prompt to delete a function entirely."""
    if not DELETE_FUNCTION_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {DELETE_FUNCTION_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = DELETE_FUNCTION_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", candidate["function"])
    prompt = prompt.replace("{{file}}", candidate["file"])
    return prompt


def build_delete_test_prompt(test_file: str, test_name: str) -> str:
    """Build prompt to delete a single test from a file."""
    if not DELETE_TEST_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {DELETE_TEST_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = DELETE_TEST_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{test_name}}", test_name)
    return prompt


def build_refactor_remove_call_prompt(function: str, test_file: str, tests_to_refactor: list[str]) -> str:
    """Build prompt to refactor tests to not call the function."""
    if not REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = REFACTOR_REMOVE_CALL_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{tests_to_refactor}}", "\n".join(f"- {t}" for t in tests_to_refactor))
    return prompt


def build_justify_helper_prompt(function: str, test_file: str) -> str:
    """Build prompt to justify whether a helper should be created."""
    if not JUSTIFY_HELPER_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {JUSTIFY_HELPER_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = JUSTIFY_HELPER_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{test_file}}", test_file)
    return prompt


def build_create_helper_prompt(function: str, source_file: str) -> str:
    """Build prompt to create a test helper from removed function."""
    if not CREATE_HELPER_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {CREATE_HELPER_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = CREATE_HELPER_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = prompt.replace("{{source_file}}", source_file)
    prompt = prompt.replace("{{helpers_dir}}", str(TEST_HELPERS_DIR))
    return prompt


def build_refactor_test_prompt(function: str, test_file: str, helper_header: str) -> str:
    """Build prompt to refactor test to use helper instead of removed function."""
    if not REFACTOR_TEST_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {REFACTOR_TEST_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = REFACTOR_TEST_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{function}}", function)
    prompt = template.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{helper_header}}", helper_header)
    return prompt


def build_cleanup_empty_tcase_prompt(test_file: str, tcase_name: str) -> str:
    """Build prompt to cleanup an empty TCase."""
    if not CLEANUP_EMPTY_TCASE_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {CLEANUP_EMPTY_TCASE_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = CLEANUP_EMPTY_TCASE_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{tcase_name}}", tcase_name)
    return prompt


def build_cleanup_unused_static_prompt(test_file: str, function_name: str) -> str:
    """Build prompt to delete a specific unused static function."""
    if not CLEANUP_UNUSED_STATICS_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {CLEANUP_UNUSED_STATICS_PROMPT_TEMPLATE}")
        sys.exit(1)

    template = CLEANUP_UNUSED_STATICS_PROMPT_TEMPLATE.read_text()
    prompt = template.replace("{{test_file}}", test_file)
    prompt = prompt.replace("{{function_name}}", function_name)
    return prompt


def parse_unused_function_errors(build_output: str) -> list[str]:
    """
    Parse build output for unused function/variable errors.
    Returns list of function names that are unused.
    """
    import re

    # Match: error: 'name' defined but not used [-Werror=unused-function] or [-Werror=unused-variable]
    # (START_TEST macro creates variables, not functions)
    pattern = re.compile(r"error: '(\w+)' defined but not used \[-Werror=unused-(?:function|variable)\]")
    return pattern.findall(build_output)


def find_empty_tcases(test_file: str) -> list[str]:
    """
    Find TCases that have no tcase_add_test calls.
    Returns list of TCase variable names that are empty.
    """
    import re

    path = Path(test_file)
    if not path.exists():
        return []

    content = path.read_text()

    # Find all TCase declarations: TCase *tc_name = tcase_create(...)
    tcase_pattern = re.compile(r'TCase\s*\*\s*(\w+)\s*=\s*tcase_create\s*\(')
    all_tcases = tcase_pattern.findall(content)

    # For each TCase, check if it has any tcase_add_test calls
    empty_tcases = []
    for tcase_name in all_tcases:
        # Look for tcase_add_test(tcase_name, ...) or tcase_add_test_raise_signal(tcase_name, ...)
        add_test_pattern = re.compile(rf'tcase_add_test(?:_raise_signal)?\s*\(\s*{re.escape(tcase_name)}\s*,')
        if not add_test_pattern.search(content):
            empty_tcases.append(tcase_name)

    return empty_tcases


def file_has_no_tcases(test_file: str) -> bool:
    """Check if a test file has no suite_add_tcase calls (empty file)."""
    import re

    path = Path(test_file)
    if not path.exists():
        return True

    content = path.read_text()
    return not re.search(r'suite_add_tcase\s*\(', content)


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, error_message).
    """
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = [
            "claude", "-p", "-",
            "--model", model,
            "--allowedTools", "Read,Edit,Write,Bash,Glob,Grep",
            "--max-turns", "20",
        ]
        env = os.environ.copy()
        if thinking_budget and thinking_budget != "0":
            env["MAX_THINKING_TOKENS"] = thinking_budget

        with open(prompt_file, 'r') as pf:
            spinner.start()
            result = subprocess.run(
                cmd,
                stdin=pf,
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=timeout,
                env=env,
            )
            spinner.stop()

        if result.returncode != 0:
            error = result.stderr.strip() if result.stderr else "unknown error"
            return False, error[:200]
        return True, ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"timeout after {timeout}s"
    finally:
        Path(prompt_file).unlink(missing_ok=True)


def invoke_claude_with_output(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str, str]:
    """
    Invoke Claude CLI and capture output.
    Returns (success, output, error_message).
    """
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = [
            "claude", "-p", "-",
            "--model", model,
            "--allowedTools", "Read",  # Read-only for analysis
            "--max-turns", "10",
        ]
        env = os.environ.copy()
        if thinking_budget and thinking_budget != "0":
            env["MAX_THINKING_TOKENS"] = thinking_budget

        with open(prompt_file, 'r') as pf:
            spinner.start()
            result = subprocess.run(
                cmd,
                stdin=pf,
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=timeout,
                env=env,
            )
            spinner.stop()

        if result.returncode != 0:
            error = result.stderr.strip() if result.stderr else "unknown error"
            return False, "", error[:200]
        return True, result.stdout, ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, "", f"timeout after {timeout}s"
    finally:
        Path(prompt_file).unlink(missing_ok=True)


def parse_justify_decision(output: str) -> tuple[str, str]:
    """
    Parse the justify-helper output.
    Returns (decision, reason) where decision is 'APPROVE', 'REJECT', or 'UNKNOWN'.
    """
    import re

    # Look for DECISION: APPROVE or DECISION: REJECT
    decision_match = re.search(r'DECISION:\s*(APPROVE|REJECT)', output, re.IGNORECASE)
    reason_match = re.search(r'REASON:\s*(.+?)(?:\n|$)', output, re.IGNORECASE)

    decision = decision_match.group(1).upper() if decision_match else "UNKNOWN"
    reason = reason_match.group(1).strip() if reason_match else "No reason provided"

    return decision, reason


def record_false_positive(function: str) -> None:
    """Record a false positive to the exclusion file and commit."""
    FALSE_POSITIVES_FILE.parent.mkdir(parents=True, exist_ok=True)

    existing = set()
    if FALSE_POSITIVES_FILE.exists():
        existing = set(FALSE_POSITIVES_FILE.read_text().strip().split('\n'))

    if function not in existing:
        with open(FALSE_POSITIVES_FILE, 'a') as f:
            f.write(f"{function}\n")
        log(f"Recorded false positive: {function}")
        # Commit the false positive so workspace stays clean
        run_cmd(["git", "add", str(FALSE_POSITIVES_FILE)])
        run_cmd(["git", "commit", "-m", f"chore: record false positive {function}"])


def run_build() -> bool:
    """Run make bin/ikigai and return success."""
    log("Verifying build...")
    spinner = Spinner("Building")
    spinner.start()
    code, _, _ = run_cmd(["make", "bin/ikigai"])
    spinner.stop()
    if code == 0:
        log("Build: PASS")
    else:
        log("Build: FAIL")
    return code == 0


def find_tests_using_function(function: str) -> list[str]:
    """Find all test files that reference the function using grep."""
    log(f"Searching for tests using {function}...")
    spinner = Spinner("Searching")
    spinner.start()

    # Grep for function calls in test files
    code, stdout, _ = run_cmd([
        "grep", "-rl",
        f"\\b{function}\\s*(",
        "tests/"
    ])
    spinner.stop()

    if code != 0 or not stdout.strip():
        log("No tests use this function")
        return []

    # Filter to only .c files
    result = sorted([f for f in stdout.strip().split('\n') if f.endswith('.c')])
    if result:
        log(f"Tests using function ({len(result)} file(s)):")
        for test_file in result:
            log(f"  - {test_file}")
    return result


def analyze_test_file(test_file: str, function: str) -> tuple[int, int, list[str]]:
    """
    Analyze a test file to count tests using the function.
    Returns (tests_using_function, total_tests, test_names_using_function).
    """
    import re

    path = Path(test_file)
    if not path.exists():
        return 0, 0, []

    content = path.read_text()

    # Find all START_TEST(test_name) declarations
    test_pattern = re.compile(r'START_TEST\s*\(\s*(\w+)\s*\)')
    all_tests = test_pattern.findall(content)
    total_tests = len(all_tests)

    # Find tests that use the function
    # Parse each test block and check if it contains the function call
    tests_using = []
    for match in test_pattern.finditer(content):
        test_name = match.group(1)
        start_pos = match.start()

        # Find the END_TEST for this test
        end_match = re.search(r'\bEND_TEST\b', content[start_pos:])
        if end_match:
            test_block = content[start_pos:start_pos + end_match.end()]
            # Check if this test block contains the function
            if re.search(rf'\b{re.escape(function)}\s*\(', test_block):
                tests_using.append(test_name)

    return len(tests_using), total_tests, tests_using


def delete_test_file(test_file: str) -> bool:
    """Delete a test file. Returns True if successful."""
    path = Path(test_file)
    if path.exists():
        path.unlink()
        log(f"Deleted: {test_file}")
        return True
    return False


def build_single_test(test_file: str) -> tuple[bool, str]:
    """Build a single test file. Returns (success, error_output)."""
    spinner = Spinner(f"Building {Path(test_file).name}")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", f"build/{test_file.replace('.c', '')}"])
    spinner.stop()
    return code == 0, stderr or stdout


def abort_with_debug(test_file: str, build_output: str) -> None:
    """Show debug info and abort for investigation."""
    log("ABORTING FOR INVESTIGATION:")
    log(f"  Files changed: {', '.join(sorted(git_get_modified_files()))}")
    log(f"  Build errors:")
    for line in build_output.strip().split('\n')[-20:]:
        log(f"    {line}")
    log("")
    log("Workspace left dirty for investigation. Run 'git diff' to see changes.")
    sys.exit(1)


def run_single_test(test_file: str) -> bool:
    """Run a single test file. Returns True if successful."""
    spinner = Spinner(f"Testing {Path(test_file).name}")
    spinner.start()
    code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_file}"])
    spinner.stop()
    return code == 0


def run_tests(test_files: list[str] | None = None) -> bool:
    """Run make check, optionally for specific test files."""
    if test_files:
        # Run only specified tests - extract test names from paths
        test_names = [Path(f).stem for f in test_files]
        test_list = " ".join(test_names)
        log(f"Verifying tests: {test_list}")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_list}"])
        spinner.stop()
    else:
        # Run all tests
        log("Verifying all tests...")
        spinner = Spinner("Testing")
        spinner.start()
        code, _, _ = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check"])
        spinner.stop()

    if code == 0:
        log("Tests: PASS")
    else:
        log("Tests: FAIL")
    return code == 0


def try_remove_function(candidate: dict, timeout: int, debug: bool = False) -> tuple[bool, str]:
    """
    Attempt to remove a dead code function.

    Flow:
    1. Comment out function, verify production builds (confirms dead)
    2. REVERT - function exists again
    3. For each test using the function:
       - Analyze: TESTING or USING?
       - If TESTING: delete test
       - If USING: refactor test
       - Verify build after each change
    4. Cleanup empty TCases
    5. Delete function from src/
    6. Verify build + tests
    7. Commit or revert

    Returns (success, reason).
    """
    function = candidate["function"]
    file = candidate["file"]

    # Defensive: ensure clean workspace before starting
    if not git_is_clean():
        log("WARNING: Dirty workspace detected - cleaning before attempt")
        git_revert()
        if not git_is_clean():
            log("ERROR: Could not clean workspace")
            return False, "dirty workspace"

    # ============================================================
    # Phase 1: Verify function is truly dead (comment out, build, revert)
    # ============================================================
    model, thinking_budget, model_name = REMOVE_MODEL
    log(f"Phase 1: Verify dead code ({model_name})")

    prompt = build_comment_out_prompt(candidate)
    success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)
    if not success:
        log(f"  {model_name} invocation FAILED: {error}")
        git_revert()
        return False, "comment out failed"

    if git_is_clean():
        log(f"  Agent made no changes")
        return False, "no changes made"

    log(f"  Files changed: {', '.join(sorted(git_get_modified_files()))}")

    # Check production build
    if not run_build():
        log(f"  False positive - function is used in production")
        git_revert()
        record_false_positive(function)
        return False, "false positive"

    log(f"  Confirmed: {function} is dead code")

    # REVERT - function exists again so tests can build
    log(f"  Reverting #if 0 - function restored for test fixes")
    git_revert()

    # ============================================================
    # Phase 2: Find and process tests one at a time
    # ============================================================
    broken_tests = find_tests_using_function(function)
    if not broken_tests:
        # No tests use this function - just delete it
        log(f"Phase 2: No tests to fix")
        log(f"Phase 3: Delete function")
        prompt = build_delete_function_prompt(candidate)
        success, error = invoke_claude(prompt, model, thinking_budget, model_name, timeout)
        if not success:
            log(f"  Delete failed: {error}")
            git_revert()
            return False, "delete failed"
        if not run_build():
            log(f"  Build failed after delete")
            git_revert()
            return False, "build failed"
        log(f"SUCCESS - removed {function}")
        git_commit(function, model_name)
        return True, "removed"

    log(f"Phase 2: Fix {len(broken_tests)} test file(s)")

    test_files_deleted = []
    test_files_modified = []
    final_model_name = model_name

    for test_file in broken_tests:
        log(f"  Processing: {test_file}")

        # Get list of tests that use the function
        tests_using, total_tests, test_names = analyze_test_file(test_file, function)
        log(f"    {tests_using}/{total_tests} tests use {function}")

        if tests_using == 0:
            log(f"    No direct usage found - skipping")
            continue

        if tests_using == total_tests:
            # ALL tests use the function - delete entire file
            log(f"    All tests affected - deleting file")
            delete_test_file(test_file)
            test_files_deleted.append(test_file)
            continue

        # Process each test individually
        for i, test_name in enumerate(test_names, 1):
            log(f"    [{i}/{len(test_names)}] {test_name}")

            # Analyze: TESTING or USING? (with escalation)
            action = None
            prompt = build_analyze_single_test_prompt(function, test_file, test_name)

            for level in range(1, len(CLASSIFY_LADDER) + 1):
                model, thinking_budget, model_name = CLASSIFY_LADDER[level]
                if level > 1:
                    log(f"      Escalating classification to {model_name}")

                success, output, error = invoke_claude_with_output(prompt, model, thinking_budget, model_name, timeout)

                if not success:
                    log(f"      Classification failed ({model_name}): {error}")
                    continue

                classification = output.strip().upper()
                if "TESTING" in classification:
                    log(f"      CLASSIFYING ({model_name}): delete")
                    action = "delete"
                    break
                elif "USING" in classification:
                    log(f"      CLASSIFYING ({model_name}): refactor")
                    action = "refactor"
                    break
                else:
                    log(f"      Unknown classification ({model_name}): {classification[:80]}")
                    continue

            if action is None:
                log(f"      Classification exhausted - skipping function")
                git_revert()
                return False, f"classification exhausted for {test_name}"

            # Perform action with escalation
            test_fixed = False
            for attempt in range(1, MAX_TEST_FIX_ATTEMPTS + 1):
                attempt_model, attempt_thinking, attempt_name = TEST_FIX_LADDER[attempt]
                if attempt > 1:
                    log(f"      Escalating to {attempt_name}")

                if action == "delete":
                    prompt = build_delete_test_prompt(test_file, test_name)
                else:
                    prompt = build_refactor_remove_call_prompt(function, test_file, [test_name])

                success, error = invoke_claude(prompt, attempt_model, attempt_thinking, attempt_name, timeout)

                if not success:
                    log(f"      LLM failed: {error}")
                    continue

                # Verify build
                build_ok, build_output = build_single_test(test_file)
                if build_ok:
                    test_fixed = True
                    final_model_name = attempt_name
                    break

                # Check for unused function errors - clean them up
                unused_funcs = parse_unused_function_errors(build_output)
                if unused_funcs:
                    log(f"      Cleaning up {len(unused_funcs)} unused static(s): {', '.join(unused_funcs)}")
                    for unused_func in unused_funcs:
                        cleanup_prompt = build_cleanup_unused_static_prompt(test_file, unused_func)
                        invoke_claude(cleanup_prompt, REMOVE_MODEL[0], REMOVE_MODEL[1], REMOVE_MODEL[2], timeout)

                    # Retry build after cleanup
                    build_ok, build_output = build_single_test(test_file)
                    if build_ok:
                        test_fixed = True
                        final_model_name = attempt_name
                        break

                if debug:
                    abort_with_debug(test_file, build_output)

                log(f"      Build failed - reverting")
                git_revert()

            if not test_fixed:
                log(f"      FAILED to {action} {test_name}")
                git_revert()
                return False, f"could not {action} {test_name}"

        test_files_modified.append(test_file)

        # Cleanup empty TCases in this file
        empty_tcases = find_empty_tcases(test_file)
        if empty_tcases:
            log(f"    Cleanup {len(empty_tcases)} empty TCase(s)")
            for tcase_name in empty_tcases:
                cleanup_prompt = build_cleanup_empty_tcase_prompt(test_file, tcase_name)
                invoke_claude(cleanup_prompt, REMOVE_MODEL[0], REMOVE_MODEL[1], REMOVE_MODEL[2], timeout)

            build_ok, build_output = build_single_test(test_file)
            if not build_ok:
                if debug:
                    abort_with_debug(test_file, build_output)
                log(f"    TCase cleanup broke build")
                git_revert()
                return False, "tcase cleanup failed"

        # Delete file if empty
        if file_has_no_tcases(test_file):
            log(f"    File empty - deleting")
            delete_test_file(test_file)
            test_files_deleted.append(test_file)
            if test_file in test_files_modified:
                test_files_modified.remove(test_file)

    # Verify all remaining tests pass
    remaining_tests = [t for t in broken_tests if t not in test_files_deleted]
    if remaining_tests:
        log(f"  Verify tests: {len(remaining_tests)} file(s)")
        if not run_tests(remaining_tests):
            log(f"  Tests failed")
            if debug:
                log("ABORTING FOR INVESTIGATION:")
                log(f"  Files changed: {', '.join(sorted(git_get_modified_files()))}")
                log("")
                log("Workspace left dirty for investigation.")
                log("Run 'make check TEST=<file>' to see test errors.")
                sys.exit(1)
            git_revert()
            return False, "tests failed"

    # ============================================================
    # Phase 3: Delete function from source
    # ============================================================
    log(f"Phase 3: Delete function from source")
    prompt = build_delete_function_prompt(candidate)
    success, error = invoke_claude(prompt, REMOVE_MODEL[0], REMOVE_MODEL[1], REMOVE_MODEL[2], timeout)
    if not success:
        log(f"  Delete failed: {error}")
        git_revert()
        return False, "delete function failed"

    # Final verification
    log(f"  Final build verification")
    if not run_build():
        log(f"  Build failed after function deletion")
        git_revert()
        return False, "final build failed"

    if remaining_tests:
        log(f"  Final test verification")
        if not run_tests(remaining_tests):
            log(f"  Tests failed after function deletion")
            if debug:
                log("ABORTING FOR INVESTIGATION:")
                log(f"  Files changed: {', '.join(sorted(git_get_modified_files()))}")
                log("")
                log("Workspace left dirty for investigation.")
                log("Run 'make check TEST=<file>' to see test errors.")
                sys.exit(1)
            git_revert()
            return False, "final tests failed"

    # ============================================================
    # Success - commit
    # ============================================================
    summary_parts = []
    if test_files_deleted:
        summary_parts.append(f"deleted {len(test_files_deleted)} test file(s)")
    if test_files_modified:
        summary_parts.append(f"modified {len(test_files_modified)} test file(s)")
    summary = ", ".join(summary_parts) if summary_parts else "no test changes"

    log(f"SUCCESS - removed {function} ({summary})")
    git_commit(function, final_model_name)
    return True, "removed"


def main() -> int:
    """Main entry point."""
    start_time = time.time()

    parser = argparse.ArgumentParser(description="Automated dead code removal loop")
    parser.add_argument("--dry-run", action="store_true",
                        help="Identify dead code without removing it")
    parser.add_argument("--function", type=str, default=None,
                        help="Only process this specific function")
    parser.add_argument("--no-spinner", action="store_true",
                        help="Disable spinner animation (useful for non-interactive output)")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    parser.add_argument("--debug", action="store_true",
                        help="On first failure, show details and abort without reverting")
    args = parser.parse_args()

    if args.no_spinner:
        Spinner.disabled = True

    opts = []
    if args.dry_run:
        opts.append("dry-run")
    if args.function:
        opts.append(f"function={args.function}")
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")
    if args.debug:
        opts.append("debug")
    log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Precondition: git must be clean
    if not git_is_clean():
        log("ERROR: Git workspace must be clean before pruning.")
        return 1

    # Get candidates
    candidates = get_dead_code_candidates()

    # Filter to specific function if requested
    if args.function:
        candidates = [c for c in candidates if c["function"] == args.function]
        if not candidates:
            log(f"Function '{args.function}' not found in dead code candidates")
            return 1

    if not candidates:
        log("No dead code found.")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    log(f"Found {len(candidates)} dead code candidates")

    # Dry-run: just list candidates and exit
    if args.dry_run:
        log("Dead code candidates:")
        for c in candidates:
            log(f"  - {c['function']} at {c['file']}:{c['line']}")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    removed = 0
    skipped = []
    fix_times: list[float] = []

    for i, candidate in enumerate(candidates, 1):
        function = candidate["function"]
        file = candidate["file"]
        line = candidate["line"]

        log(f"[{i}/{len(candidates)}] {function} at {file}:{line}")

        fix_start = time.time()
        success, reason = try_remove_function(candidate, args.time_out, args.debug)
        fix_elapsed = time.time() - fix_start
        fix_times.append(fix_elapsed)

        if success:
            removed += 1
        else:
            skipped.append(f"{function} - {reason}")

        avg_time = sum(fix_times) / len(fix_times)
        remaining = len(candidates) - i
        eta = avg_time * remaining
        log(f"elapsed: {format_elapsed(fix_elapsed)} | ETA: {format_elapsed(eta)}")

    # Summary
    log("")
    log("/prune complete")
    log("")
    log(f"Removed: {removed}")
    log(f"Skipped: {len(skipped)}")

    if skipped:
        log("")
        log("Skipped functions:")
        for s in skipped:
            log(f"  - {s}")

    log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")

    return 0 if removed > 0 or len(skipped) == 0 else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
