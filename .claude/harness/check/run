#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/check - Automated test fix loop

Runs make check, fixes failures one at a time with escalation,
commits on success, reverts on exhaustion, continues until
all pass or no progress is made.
"""

import argparse
import subprocess
import sys
import os
import json
import threading
import time
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path

# Configuration
MAX_ATTEMPTS_PER_FILE = 3

# Escalation ladder: (model, thinking_budget, display_name)
ESCALATION_LADDER = {
    1: ("claude-sonnet-4-20250514", "10000", "sonnet:think"),
    2: ("claude-opus-4-20250514", "10000", "opus:think"),
    3: ("claude-opus-4-20250514", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
FIX_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
HISTORY_FILE = SCRIPT_DIR / "history.md"
REPORTS_DIR = PROJECT_ROOT / "reports" / "check"
DEFAULT_TIMEOUT = 600

# Global flag to disable spinner (set by --no-spinner)
SPINNER_ENABLED = True


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        if not SPINNER_ENABLED:
            return
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'check':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None, timeout: int | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
        timeout=timeout,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def run_make_check() -> tuple[bool, str]:
    """Run make check and return (success, output)."""
    log("Running make check...")
    spinner = Spinner("Running make check")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check"], timeout=1800)
    spinner.stop()
    output = stdout + stderr
    return code == 0, output


def run_single_test(test_file: str) -> tuple[bool, str]:
    """Run a single test file and return (success, output)."""
    test_name = Path(test_file).stem

    spinner = Spinner(f"Running {test_name}")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", "-j1", "MAKE_JOBS=1", "check", f"TEST={test_file}"], timeout=1800)
    spinner.stop()
    return code == 0, stdout + stderr


def parse_failures_from_xml() -> list[dict]:
    """
    Parse XML reports from reports/check/ to extract failing tests.
    Returns list of {file, function, line, message} dicts sorted by (file, function).
    """
    failures = []

    if not REPORTS_DIR.exists():
        return failures

    # XML namespace used by check framework
    ns = {"check": "http://check.sourceforge.net/ns"}

    for xml_file in REPORTS_DIR.rglob("*.xml"):
        try:
            tree = ET.parse(xml_file)
            root = tree.getroot()

            for test in root.findall(".//check:test[@result='failure']", ns):
                path_elem = test.find("check:path", ns)
                fn_elem = test.find("check:fn", ns)
                id_elem = test.find("check:id", ns)
                msg_elem = test.find("check:message", ns)

                if path_elem is None or fn_elem is None or id_elem is None:
                    continue

                # fn format: "basic_test.c:156"
                fn_parts = fn_elem.text.split(":")
                filename = fn_parts[0]
                line = fn_parts[1] if len(fn_parts) > 1 else "1"

                # Build full file path: path + filename
                file_path = f"{path_elem.text}/{filename}"

                failures.append({
                    "file": file_path,
                    "function": id_elem.text,
                    "line": line,
                    "message": msg_elem.text if msg_elem is not None else "test failure",
                })

        except ET.ParseError as e:
            log(f"Warning: Failed to parse {xml_file}: {e}")
            continue

    # Sort by (file, function) for consistent ordering
    failures.sort(key=lambda f: (f["file"], f["function"]))
    return failures


def load_prompt_template() -> str:
    """Load the fix prompt template."""
    if not FIX_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {FIX_PROMPT_TEMPLATE}")
        sys.exit(1)
    return FIX_PROMPT_TEMPLATE.read_text()


def process_conditionals(template: str, variables: dict) -> str:
    """
    Process Handlebars-style {{#if var}}...{{/if}} conditionals.
    Includes block content if variable is truthy, removes it otherwise.
    """
    import re
    # Pattern: {{#if var_name}}...content...{{/if}}
    pattern = re.compile(r'\{\{#if\s+(\w+)\}\}(.*?)\{\{/if\}\}', re.DOTALL)

    def replacer(match: re.Match) -> str:
        var_name = match.group(1)
        content = match.group(2)
        if variables.get(var_name):
            return content
        return ""

    return pattern.sub(replacer, template)


def build_prompt(failure: dict) -> str:
    """Build the fix prompt from template."""
    template = load_prompt_template()

    # Build variables dict for conditional processing
    variables = {
        "file": failure["file"],
        "function": failure["function"],
        "line": failure["line"],
        "message": failure["message"],
        "history": load_history(),
    }

    # Process conditionals first
    prompt = process_conditionals(template, variables)

    # Then substitute variables
    for key, value in variables.items():
        prompt = prompt.replace("{{" + key + "}}", value)

    return prompt


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, response).
    """
    import tempfile

    # Write prompt to temp file to avoid command line length limits
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        # Use shell to pipe the prompt file
        # Set MAX_THINKING_TOKENS env var for extended thinking
        cmd = f'cat "{prompt_file}" | claude -p - --model {model} --allowedTools "Read,Edit,Write,Bash,Glob,Grep" --output-format json --max-turns 20'
        env = os.environ.copy()
        env["MAX_THINKING_TOKENS"] = thinking_budget
        spinner.start()
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=PROJECT_ROOT,
            timeout=timeout,
            env=env,
        )
        spinner.stop()
        code = result.returncode
        stdout = result.stdout or ""
        stderr = result.stderr or ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"Timeout after {timeout} seconds"
    finally:
        Path(prompt_file).unlink(missing_ok=True)

    if code != 0:
        return False, stderr

    try:
        result = json.loads(stdout)
        return True, result.get("result", "")
    except json.JSONDecodeError:
        return True, stdout


def jj_get_modified_files() -> set[str]:
    """Get set of currently modified files."""
    code, stdout, _ = run_cmd(["jj", "diff", "--summary"])
    files = set()
    for line in stdout.strip().split('\n'):
        if line.strip():
            # Format: "A filename" or "M filename" or "D filename"
            parts = line.split(None, 1)
            if len(parts) == 2:
                files.add(parts[1])
    return files


def jj_has_changes() -> bool:
    """Check if there are uncommitted changes."""
    return bool(jj_get_modified_files())


def jj_commit(failure: dict, model_name: str, attempt: int, files_before: set[str]) -> bool:
    """Commit only files that changed during fix attempt."""
    files_after = jj_get_modified_files()
    new_changes = files_after - files_before

    if not new_changes:
        return False

    # jj auto-tracks all files, no add needed
    file_name = Path(failure["file"]).name
    func_name = failure["function"]
    msg = f"fix: {file_name}::{func_name}\n\nharness/check | {model_name} | attempt {attempt}"

    code, _, _ = run_cmd(["jj", "commit", "-m", msg])
    return code == 0


def jj_revert(files_before: set[str] | None = None) -> None:
    """Revert changes made during fix attempt.

    If files_before is provided, only reverts files that were newly modified.
    Otherwise reverts all uncommitted changes.
    """
    if files_before is not None:
        files_after = jj_get_modified_files()
        new_changes = files_after - files_before
        if new_changes:
            log("Reverting uncommitted changes...")
            for f in new_changes:
                run_cmd(["jj", "restore", f])
    elif jj_has_changes():
        log("Reverting uncommitted changes...")
        run_cmd(["jj", "restore"])


def truncate_history() -> None:
    """Truncate history file at start of new file's fix attempts."""
    HISTORY_FILE.write_text("")


def load_history() -> str:
    """Load history content, returning empty string if file doesn't exist."""
    if HISTORY_FILE.exists():
        return HISTORY_FILE.read_text()
    return ""


def try_fix_failure(failure: dict, timeout: int) -> bool:
    """
    Attempt to fix a single failing test function.
    Returns True if fixed, False if exhausted attempts.
    """
    file = failure["file"]
    func = failure["function"]

    truncate_history()
    # Capture files before any fix attempts
    files_before = jj_get_modified_files()

    for attempt in range(1, MAX_ATTEMPTS_PER_FILE + 1):
        model, thinking_budget, model_name = ESCALATION_LADDER[attempt]

        log(f"Trying {model_name} (attempt {attempt}/{MAX_ATTEMPTS_PER_FILE})")

        prompt = build_prompt(failure)
        success, response = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

        if not success:
            log(f"{model_name} invocation FAILED: {response[:200]}")
            continue

        # Verify the fix by running the test and checking XML for this specific function
        test_passed, _ = run_single_test(file)

        if test_passed:
            log(f"{model_name} SUCCESS")
            jj_commit(failure, model_name, attempt, files_before)
            return True

        # Check if this specific function now passes (even if others still fail)
        if check_function_passes(file, func):
            log(f"{model_name} SUCCESS (function fixed)")
            jj_commit(failure, model_name, attempt, files_before)
            return True

        log(f"{model_name} FAILED (test still failing)")

    # Exhausted all attempts - revert only changes from this fix attempt
    log(f"SKIPPED - exhausted {MAX_ATTEMPTS_PER_FILE} attempts")
    jj_revert(files_before)
    return False


def check_function_passes(file: str, function: str) -> bool:
    """Check if a specific test function now passes by examining its XML report."""
    # Derive XML path from file path
    # file: tests/unit/array/basic_test.c -> reports/check/unit/array/basic_test.xml
    file_path = Path(file)
    relative = file_path.relative_to("tests")
    xml_path = REPORTS_DIR / relative.with_suffix(".xml")

    if not xml_path.exists():
        return False

    ns = {"check": "http://check.sourceforge.net/ns"}

    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        for test in root.findall(".//check:test", ns):
            id_elem = test.find("check:id", ns)
            if id_elem is not None and id_elem.text == function:
                return test.get("result") == "success"

    except ET.ParseError:
        pass

    return False


def drop_test_databases() -> None:
    """Drop all test databases to start fresh."""
    log("Cleaning test databases...")

    # Get list of test databases
    spinner = Spinner("Finding test databases")
    spinner.start()
    code, stdout, stderr = run_cmd([
        "psql",
        "postgresql://ikigai:ikigai@localhost/postgres",
        "-t",  # Tuples only (no headers)
        "-c",
        "SELECT datname FROM pg_database WHERE datname LIKE 'ikigai_test_%';"
    ])
    spinner.stop()

    if code != 0:
        log(f"Warning: Could not list test databases: {stderr}")
        return

    # Parse database names from output
    db_names = [line.strip() for line in stdout.strip().split('\n') if line.strip()]

    if not db_names:
        log("No test databases to clean")
        return

    log(f"Dropping {len(db_names)} test database(s)...")

    # Drop each database
    for db_name in db_names:
        spinner = Spinner(f"Dropping {db_name}")
        spinner.start()
        code, _, stderr = run_cmd([
            "psql",
            "postgresql://ikigai:ikigai@localhost/postgres",
            "-c",
            f"DROP DATABASE IF EXISTS {db_name};"
        ])
        spinner.stop()

        if code == 0:
            log(f"Dropped {db_name}")
        else:
            log(f"Warning: Failed to drop {db_name}: {stderr}")


def main() -> int:
    """Main entry point."""
    start_time = time.time()

    parser = argparse.ArgumentParser(description="Automated test fix loop")
    parser.add_argument("--dry-run", action="store_true",
                        help="Identify failures without fixing them")
    parser.add_argument("--no-spinner", action="store_true",
                        help="Disable progress spinner (for non-interactive use)")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    args = parser.parse_args()

    global SPINNER_ENABLED
    if args.no_spinner:
        SPINNER_ENABLED = False

    opts = []
    if args.dry_run:
        opts.append("dry-run")
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")
    log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Clean build to start fresh
    log("Running make clean...")
    spinner = Spinner("Running make clean")
    spinner.start()
    run_cmd(["make", "clean"])
    spinner.stop()

    # Clean test databases
    drop_test_databases()

    # Run full make check
    success, output = run_make_check()

    if success:
        log(f"All tests pass! (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    # Parse failures from XML reports
    failures = parse_failures_from_xml()

    if not failures:
        log("make check failed but couldn't parse failures from XML")
        log("Output tail:")
        for line in output.split('\n')[-20:]:
            log(f"  {line}")
        log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
        return 1

    log(f"Found {len(failures)} failing test functions")

    # Dry-run: just list failures and exit
    if args.dry_run:
        log("Test functions needing fixes:")
        for failure in failures:
            log(f"  - {failure['file']}:{failure['line']} {failure['function']} - {failure['message'][:40]}")
        log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    pass_num = 0

    while True:
        pass_num += 1
        log(f"=== Pass {pass_num} ===")

        if pass_num > 1:
            # Re-run check for subsequent passes
            success, output = run_make_check()

            if success:
                log(f"All tests pass! (elapsed: {format_elapsed(time.time() - start_time)})")
                return 0

            failures = parse_failures_from_xml()

            if not failures:
                log("make check failed but couldn't parse failures from XML")
                log("Output tail:")
                for line in output.split('\n')[-20:]:
                    log(f"  {line}")
                log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
                return 1

            log(f"Found {len(failures)} failing test functions")

        fixed_count = 0
        skipped = []
        fix_times: list[float] = []

        for i, failure in enumerate(failures, 1):
            file = failure["file"]
            func = failure["function"]
            msg = failure["message"][:40]
            log(f"[{i}/{len(failures)}] {file}:{failure['line']} {func} - {msg}")

            fix_start = time.time()
            if try_fix_failure(failure, args.time_out):
                fixed_count += 1
            else:
                skipped.append(f"{file}::{func}")
            fix_elapsed = time.time() - fix_start
            fix_times.append(fix_elapsed)
            avg_time = sum(fix_times) / len(fix_times)
            remaining = len(failures) - i
            eta = avg_time * remaining
            log(f"elapsed: {format_elapsed(fix_elapsed)} | ETA: {format_elapsed(eta)}")

        log(f"Pass {pass_num} complete: {fixed_count} fixed, {len(skipped)} skipped")

        if fixed_count > 0 and len(skipped) == 0:
            log(f"All failures fixed! (elapsed: {format_elapsed(time.time() - start_time)})")
            return 0

        if fixed_count == 0:
            log("No progress made. Stopping.")
            if skipped:
                log("Skipped test functions:")
                for f in skipped:
                    log(f"  - {f}")
            log(f"Completed with failures (elapsed: {format_elapsed(time.time() - start_time)})")
            return 1

        # Progress made but some failures remain, continue to next pass

    log(f"Completed (elapsed: {format_elapsed(time.time() - start_time)})")
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
