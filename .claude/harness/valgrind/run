#!/usr/bin/env python3
import sys
sys.dont_write_bytecode = True
"""
harness/valgrind - Valgrind Memcheck checker and automated fix loop

Default behavior: Run check-valgrind, parse results, output JSON and exit.
With --fix: Run automated fix loop with escalation ladder.
"""

import argparse
import json
import subprocess
import sys
import os
import re
import threading
import time
from datetime import datetime
from pathlib import Path

# Configuration
MAX_ATTEMPTS_PER_FILE = 3

# Escalation ladder: (model, thinking_budget, display_name)
ESCALATION_LADDER = {
    1: ("claude-sonnet-4-20250514", "10000", "sonnet:think"),
    2: ("claude-opus-4-20250514", "10000", "opus:think"),
    3: ("claude-opus-4-20250514", "128000", "opus:ultrathink"),
}

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
FIX_PROMPT_TEMPLATE = SCRIPT_DIR / "fix.prompt.md"
HISTORY_FILE = SCRIPT_DIR / "history.md"
DEFAULT_TIMEOUT = 900

# Global flag to disable spinner (set by --verbose to enable)
SPINNER_ENABLED = False
SILENT_MODE = True


class Spinner:
    """Threaded spinner for long-running operations."""

    FRAMES = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]

    def __init__(self, message: str = ""):
        self.message = message
        self.running = False
        self.thread: threading.Thread | None = None
        self.start_time: float = 0

    def _spin(self) -> None:
        idx = 0
        while self.running:
            elapsed = int(time.time() - self.start_time)
            mins, secs = divmod(elapsed, 60)
            frame = self.FRAMES[idx % len(self.FRAMES)]
            sys.stdout.write(f"\r{frame} {self.message} [{mins:02d}:{secs:02d}]")
            sys.stdout.flush()
            idx += 1
            time.sleep(0.1)
        sys.stdout.write("\r" + " " * 60 + "\r")
        sys.stdout.flush()

    def start(self) -> None:
        if not SPINNER_ENABLED:
            return
        self.running = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._spin, daemon=True)
        self.thread.start()

    def stop(self) -> None:
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)


def log(msg: str) -> None:
    """Print timestamped log message."""
    if SILENT_MODE:
        return
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{timestamp} | {'valgrind':10} | {msg}", flush=True)


def format_elapsed(seconds: float) -> str:
    """Format elapsed seconds as human-readable string."""
    hours, remainder = divmod(int(seconds), 3600)
    mins, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}h {mins}m {secs}s"
    elif mins > 0:
        return f"{mins}m {secs}s"
    else:
        return f"{secs}s"


def run_cmd(cmd: list[str], capture: bool = True, cwd: Path | None = None, timeout: int | None = None) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        capture_output=capture,
        text=True,
        cwd=cwd or PROJECT_ROOT,
        timeout=timeout,
    )
    return result.returncode, result.stdout or "", result.stderr or ""


def drop_test_databases() -> None:
    """Drop all test databases (ikigai_test_*)."""
    log("Cleaning up test databases...")

    # Get list of test databases
    list_cmd = [
        "psql",
        "postgresql://ikigai:ikigai@localhost/postgres",
        "-t",  # tuple-only output
        "-c",
        "SELECT datname FROM pg_database WHERE datname LIKE 'ikigai_test_%';"
    ]

    code, stdout, stderr = run_cmd(list_cmd)
    if code != 0:
        log(f"Warning: Failed to list test databases: {stderr.strip()}")
        return

    # Parse database names
    db_names = [line.strip() for line in stdout.strip().split('\n') if line.strip()]

    if not db_names:
        log("No test databases to clean up")
        return

    log(f"Found {len(db_names)} test database(s) to drop")

    # Drop each database
    for db_name in db_names:
        drop_cmd = [
            "psql",
            "postgresql://ikigai:ikigai@localhost/postgres",
            "-c",
            f"DROP DATABASE IF EXISTS {db_name};"
        ]
        code, _, stderr = run_cmd(drop_cmd)
        if code == 0:
            log(f"Dropped {db_name}")
        else:
            log(f"Warning: Failed to drop {db_name}: {stderr.strip()}")


def run_make_valgrind(quiet: bool = False) -> tuple[bool, str]:
    """Run make check-valgrind and return (success, output)."""
    if not quiet:
        log("Running make check-valgrind...")
    spinner = Spinner("Running make check-valgrind")
    spinner.start()
    code, stdout, stderr = run_cmd(["make", "check-valgrind", "MAKE_JOBS=1"], timeout=3600)
    spinner.stop()
    output = stdout + stderr
    return code == 0, output


def parse_failing_tests(output: str) -> list[str]:
    """
    Parse make output for ðŸ”´ lines to extract failing test binary paths.
    Returns list of binary paths like 'build-valgrind/tests/unit/foo_test'.
    """
    failing_tests = []
    for line in output.split('\n'):
        line = line.strip()
        if line.startswith('ðŸ”´ '):
            binary_path = line[2:].strip()
            if binary_path:
                failing_tests.append(binary_path)
    return failing_tests


def run_single_test(test_binary: str) -> tuple[bool, str]:
    """
    Run a single test binary under valgrind to capture memcheck output.
    Returns (success, output).
    """
    test_name = Path(test_binary).name
    spinner = Spinner(f"Running {test_name}")
    spinner.start()

    env = os.environ.copy()
    env["CK_FORK"] = "no"
    env["CK_TIMEOUT_MULTIPLIER"] = "10"

    result = subprocess.run(
        ["valgrind", "--error-exitcode=1", "--leak-check=full", test_binary],
        capture_output=True,
        text=True,
        cwd=PROJECT_ROOT,
        timeout=600,
        env=env,
    )
    spinner.stop()

    output = (result.stdout or "") + (result.stderr or "")
    return result.returncode == 0, output


def parse_failures(output: str) -> list[dict]:
    """
    Parse Valgrind output to extract memory errors.
    Returns list of {file, line, error_type, message, stack} dicts.
    """
    failures = []
    seen_files = set()

    # Valgrind error patterns:
    # ==PID== Invalid read of size N
    # ==PID== Conditional jump or move depends on uninitialised value(s)
    # ==PID== Use of uninitialised value of size N
    # ==PID== N bytes in M blocks are definitely lost
    error_patterns = [
        (re.compile(r'==\d+==\s+(Invalid (?:read|write) of size \d+)'), 'memory error'),
        (re.compile(r'==\d+==\s+(Conditional jump or move depends on uninitialised value)'), 'uninitialized'),
        (re.compile(r'==\d+==\s+(Use of uninitialised value of size \d+)'), 'uninitialized'),
        (re.compile(r'==\d+==\s+(\d+ bytes in \d+ blocks are (?:definitely|indirectly|possibly) lost)'), 'memory leak'),
        (re.compile(r'==\d+==\s+(Invalid free)'), 'invalid free'),
        (re.compile(r'==\d+==\s+(Mismatched free)'), 'mismatched free'),
    ]

    # Location pattern: ==PID==    at/by 0x...: function (file.c:123)
    location_pattern = re.compile(r'==\d+==\s+(?:at|by)\s+0x[0-9A-Fa-f]+:\s+\S+\s+\((\S+):(\d+)\)')

    lines = output.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i]

        for pattern, error_category in error_patterns:
            match = pattern.search(line)
            if match:
                error_type = match.group(1)
                stack_lines = []

                # Collect stack trace (next 20 lines)
                for j in range(i + 1, min(i + 20, len(lines))):
                    stack_lines.append(lines[j])

                    loc_match = location_pattern.search(lines[j])
                    if loc_match:
                        file = loc_match.group(1)
                        line_num = loc_match.group(2)
                        if file not in seen_files and (file.startswith('src/') or file.startswith('tests/')):
                            seen_files.add(file)
                            failures.append({
                                "file": file,
                                "line": line_num,
                                "error_type": f"Valgrind: {error_category}",
                                "message": error_type,
                                "stack": '\n'.join(stack_lines[:15]),
                            })
                            break
                break

        i += 1

    return failures


def load_prompt_template() -> str:
    """Load the fix prompt template."""
    if not FIX_PROMPT_TEMPLATE.exists():
        log(f"ERROR: Missing {FIX_PROMPT_TEMPLATE}")
        sys.exit(1)
    return FIX_PROMPT_TEMPLATE.read_text()


def process_conditionals(template: str, variables: dict) -> str:
    """
    Process Handlebars-style {{#if var}}...{{/if}} conditionals.
    Includes block content if variable is truthy, removes it otherwise.
    """
    # Pattern: {{#if var_name}}...content...{{/if}}
    pattern = re.compile(r'\{\{#if\s+(\w+)\}\}(.*?)\{\{/if\}\}', re.DOTALL)

    def replacer(match: re.Match) -> str:
        var_name = match.group(1)
        content = match.group(2)
        if variables.get(var_name):
            return content
        return ""

    return pattern.sub(replacer, template)


def build_prompt(failure: dict, make_output: str) -> str:
    """Build the fix prompt from template."""
    template = load_prompt_template()

    # Build variables dict for conditional processing
    variables = {
        "file": failure["file"],
        "line": failure["line"],
        "error_type": failure["error_type"],
        "message": failure["message"],
        "stack": failure["stack"],
        "make_output": make_output[-6000:],
        "history": load_history(),
    }

    # Process conditionals first
    prompt = process_conditionals(template, variables)

    # Then substitute variables
    for key, value in variables.items():
        prompt = prompt.replace("{{" + key + "}}", value)

    return prompt


def invoke_claude(prompt: str, model: str, thinking_budget: str, model_name: str, timeout: int) -> tuple[bool, str]:
    """
    Invoke Claude CLI with the prompt via stdin.
    Returns (success, response).
    """
    import json
    import tempfile

    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(prompt)
        prompt_file = f.name

    spinner = Spinner(f"Waiting for {model_name}")
    try:
        cmd = f'cat "{prompt_file}" | claude -p - --model {model} --allowedTools "Read,Edit,Write,Bash,Glob,Grep" --output-format json --max-turns 30'
        env = os.environ.copy()
        env["MAX_THINKING_TOKENS"] = thinking_budget
        spinner.start()
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=PROJECT_ROOT,
            timeout=timeout,
            env=env,
        )
        spinner.stop()
        code = result.returncode
        stdout = result.stdout or ""
        stderr = result.stderr or ""
    except subprocess.TimeoutExpired:
        spinner.stop()
        return False, f"Timeout after {timeout} seconds"
    finally:
        Path(prompt_file).unlink(missing_ok=True)

    if code != 0:
        return False, stderr

    try:
        result = json.loads(stdout)
        return True, result.get("result", "")
    except json.JSONDecodeError:
        return True, stdout


def jj_get_modified_files() -> set[str]:
    """Get set of currently modified files."""
    code, stdout, _ = run_cmd(["jj", "diff", "--summary"])
    files = set()
    for line in stdout.strip().split('\n'):
        if line.strip():
            parts = line.split(None, 1)
            if len(parts) == 2:
                files.add(parts[1])
    return files


def jj_commit(failure: dict, model_name: str, attempt: int, files_before: set[str]) -> bool:
    """Commit only files that changed during fix attempt."""
    files_after = jj_get_modified_files()
    new_changes = files_after - files_before

    if not new_changes:
        return False

    file_name = Path(failure["file"]).name
    msg = f"fix: memory error in {file_name}\n\nharness/valgrind | {model_name} | attempt {attempt}"

    code, _, _ = run_cmd(["jj", "commit", "-m", msg])
    return code == 0


def jj_revert(files_before: set[str] | None = None) -> None:
    """Revert changes made during fix attempt."""
    if files_before is not None:
        files_after = jj_get_modified_files()
        new_changes = files_after - files_before
        if new_changes:
            log("Reverting uncommitted changes...")
            for f in new_changes:
                run_cmd(["jj", "restore", f])
    else:
        if jj_get_modified_files():
            log("Reverting uncommitted changes...")
            run_cmd(["jj", "restore"])


def truncate_history() -> None:
    """Truncate history file at start of new file's fix attempts."""
    HISTORY_FILE.write_text("")


def load_history() -> str:
    """Load history content, returning empty string if file doesn't exist."""
    if HISTORY_FILE.exists():
        return HISTORY_FILE.read_text()
    return ""


def try_fix_file(failure: dict, make_output: str, timeout: int) -> bool:
    """
    Attempt to fix a memory error.
    Returns True if fixed, False if exhausted attempts.
    """
    file = failure["file"]

    truncate_history()
    files_before = jj_get_modified_files()

    for attempt in range(1, MAX_ATTEMPTS_PER_FILE + 1):
        model, thinking_budget, model_name = ESCALATION_LADDER[attempt]

        log(f"Trying {model_name} (attempt {attempt}/{MAX_ATTEMPTS_PER_FILE})")

        prompt = build_prompt(failure, make_output)
        success, response = invoke_claude(prompt, model, thinking_budget, model_name, timeout)

        if not success:
            log(f"{model_name} invocation FAILED: {response[:200]}")
            continue

        # Verify the fix
        check_passed, _ = run_make_valgrind()

        if check_passed:
            log(f"{model_name} SUCCESS")
            jj_commit(failure, model_name, attempt, files_before)
            return True
        else:
            log(f"{model_name} FAILED (memory errors remain)")

    # Exhausted all attempts
    log(f"SKIPPED - exhausted {MAX_ATTEMPTS_PER_FILE} attempts")
    jj_revert(files_before)
    return False


def format_error_item(failure: dict) -> str:
    """Format a failure as a human-readable error item for JSON output."""
    error_type = failure.get("message", "Unknown error")
    file = failure.get("file", "unknown")
    line = failure.get("line", "?")
    return f"{error_type} at {file}:{line}"


def output_json_result(ok: bool, failures: list[dict] | None = None) -> int:
    """Output JSON result to stdout and return appropriate exit code."""
    if ok:
        result = {"ok": True}
    else:
        items = [format_error_item(f) for f in (failures or [])]
        result = {"ok": False, "items": items}
    print(json.dumps(result))
    return 0 if ok else 1


def main() -> int:
    """Main entry point."""
    start_time = time.time()
    parser = argparse.ArgumentParser(description="Valgrind Memcheck checker and automated fix loop")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable progress output and spinner")
    parser.add_argument("--time-out", type=int, default=DEFAULT_TIMEOUT,
                        help=f"Timeout in seconds for each LLM invocation (default: {DEFAULT_TIMEOUT})")
    parser.add_argument("--file", type=str, default=None,
                        help="Filter results to a specific file")
    args = parser.parse_args()

    global SPINNER_ENABLED, SILENT_MODE
    if args.verbose:
        SPINNER_ENABLED = True
        SILENT_MODE = False

    args.fix = False

    opts = []
    if args.time_out != DEFAULT_TIMEOUT:
        opts.append(f"timeout={args.time_out}s")

    # Only log in fix mode
    if args.fix:
        log("Starting" + (f" ({', '.join(opts)})" if opts else ""))
    os.chdir(PROJECT_ROOT)

    # Clean up test databases before running checks (only in fix mode to avoid noise)
    if args.fix:
        drop_test_databases()

    # Step 1: Run make check-valgrind to get list of failing test binaries
    success, make_output = run_make_valgrind(quiet=not args.fix)

    if success:
        # Check-only mode: output JSON result
        if not args.fix:
            return output_json_result(ok=True)
        log(f"All Valgrind checks passed! (elapsed: {format_elapsed(time.time() - start_time)})")
        return 0

    # Step 2: Parse failing test binaries from ðŸ”´ lines
    failing_tests = parse_failing_tests(make_output)

    if not failing_tests:
        if not args.fix:
            # Can't parse failures, output generic error
            return output_json_result(ok=False, failures=[{"message": "make check-valgrind failed", "file": "unknown", "line": "?"}])
        log("make check-valgrind failed but couldn't find failing tests")
        log("Output tail:")
        for line in make_output.split('\n')[-30:]:
            log(f"  {line}")
        log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
        return 1

    if args.fix:
        log(f"Found {len(failing_tests)} failing test binaries")

    # Step 3: For each failing test, run it under valgrind to get output
    all_failures = []
    for test_binary in failing_tests:
        test_name = Path(test_binary).name
        if args.fix:
            log(f"Analyzing {test_name}...")
        _, test_output = run_single_test(test_binary)
        failures = parse_failures(test_output)
        for f in failures:
            f["test_binary"] = test_binary
            f["test_output"] = test_output
        all_failures.extend(failures)

    if not all_failures:
        if not args.fix:
            # Tests failed but couldn't parse specific errors
            return output_json_result(ok=False, failures=[{"message": "Valgrind errors detected", "file": "unknown", "line": "?"}])
        log("Tests failed but couldn't parse Valgrind errors from output")
        log(f"Completed with errors (elapsed: {format_elapsed(time.time() - start_time)})")
        return 1

    # Check-only mode (no --fix): output JSON and exit
    if not args.fix:
        # Filter failures to target file if --file is specified
        if args.file:
            target_file = args.file
            # Check if target file exists
            if not Path(target_file).exists():
                result = {"ok": False, "items": [f"{target_file}: file not found"]}
                print(json.dumps(result))
                return 1
            # Filter to errors that mention the target file in the file field
            all_failures = [f for f in all_failures if target_file in f.get("file", "")]
            # If no failures for this file, it's a success
            if not all_failures:
                result = {"ok": True}
                print(json.dumps(result))
                return 0
        return output_json_result(ok=False, failures=all_failures)

    log(f"Found {len(all_failures)} memory errors across {len(failing_tests)} test(s)")

    # --fix mode: Run the fix loop
    pass_num = 0

    while True:
        pass_num += 1
        log(f"=== Pass {pass_num} ===")

        # Step 4: Fix each failure
        fixed_count = 0
        skipped = []
        file_times: list[float] = []

        for i, failure in enumerate(all_failures, 1):
            file = failure["file"]
            error = failure["error_type"]
            log(f"[{i}/{len(all_failures)}] {file}:{failure['line']} - {error}")

            file_start = time.time()
            if try_fix_file(failure, failure["test_output"], args.time_out):
                fixed_count += 1
            else:
                skipped.append(f"{file}:{failure['line']}")
            file_elapsed = time.time() - file_start
            file_times.append(file_elapsed)
            avg_time = sum(file_times) / len(file_times)
            remaining = len(all_failures) - i
            eta = avg_time * remaining
            log(f"elapsed: {format_elapsed(file_elapsed)} | ETA: {format_elapsed(eta)}")

        log(f"Pass {pass_num} complete: {fixed_count} fixed, {len(skipped)} skipped")

        if fixed_count > 0 and len(skipped) == 0:
            log(f"All failures fixed! (elapsed: {format_elapsed(time.time() - start_time)})")
            return 0

        if fixed_count == 0:
            log("No progress made. Stopping.")
            if skipped:
                log("Skipped errors:")
                for f in skipped:
                    log(f"  - {f}")
            log(f"Completed with failures (elapsed: {format_elapsed(time.time() - start_time)})")
            return 1

        # Re-run check to find remaining failures for next pass
        drop_test_databases()
        success, make_output = run_make_valgrind()

        if success:
            log(f"All Valgrind checks passed! (elapsed: {format_elapsed(time.time() - start_time)})")
            return 0

        failing_tests = parse_failing_tests(make_output)
        if not failing_tests:
            log("make check-valgrind failed but couldn't find failing tests")
            return 1

        log(f"Found {len(failing_tests)} failing test binaries")

        all_failures = []
        for test_binary in failing_tests:
            test_name = Path(test_binary).name
            log(f"Analyzing {test_name}...")
            _, test_output = run_single_test(test_binary)
            failures = parse_failures(test_output)
            for f in failures:
                f["test_binary"] = test_binary
                f["test_output"] = test_output
            all_failures.extend(failures)

        if not all_failures:
            log("Tests failed but couldn't parse Valgrind errors from output")
            return 1

        log(f"Found {len(all_failures)} memory errors across {len(failing_tests)} test(s)")


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit(130)
